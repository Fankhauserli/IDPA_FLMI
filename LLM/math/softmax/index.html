<!doctype html>
<html lang="de">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>IDPA FLMI — Softmax-Funktion</title>
    <meta name="description" content="Erklärung der Softmax-Funktion und ihrer Anwendung in LLMs" />
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="../../../assets/css/styles.css" />
</head>
<body class="antialiased bg-gray-50 text-gray-800">
    <header class="bg-white shadow">
        <div class="max-w-6xl mx-auto px-4 sm:px-6 py-6">
            <a href="/" class="text-indigo-600 hover:underline">← Zurück zur Übersicht</a>
            <h1 class="text-3xl font-bold mt-2">Softmax-Funktion</h1>
            <p class="text-gray-600 mt-1">Wie Wahrscheinlichkeiten in LLMs berechnet werden</p>
        </div>
    </header>
    <main class="max-w-4xl mx-auto px-4 py-12">
        <div class="bg-white p-8 rounded-lg shadow space-y-8">

            <div>
                <p class="text-lg leading-relaxed">Die Softmax-Funktion ist eine zentrale Aktivierungsfunktion in neuronalen Netzen, insbesondere in der Klassifikation und bei der Vorhersage von Wahrscheinlichkeiten. Sie wandelt einen Vektor von beliebigen reellen Zahlen in einen Wahrscheinlichkeitsvektor um, dessen Elemente zwischen 0 und 1 liegen und sich zu 1 summieren.</p>
            </div>



            <div class="border-t pt-8">
                <h2 class="text-2xl font-bold mb-4">Eigenschaften und Vorteile</h2>
                <ul class="list-disc list-inside mt-4 pl-4 space-y-2">
                    <li><strong>Wahrscheinlichkeitsverteilung:</strong> Die Ausgabe ist eine diskrete Wahrscheinlichkeitsverteilung über die möglichen Klassen.</li>
                    <li><strong>Monotonie:</strong> Grössere Eingabewerte führen zu grösseren Wahrscheinlichkeiten.</li>
                    <li><strong>"Soft" Max:</strong> Im Gegensatz zur "Hardmax"-Funktion, die nur die Klasse mit dem höchsten Wert auswählt, gibt Softmax eine weiche Zuweisung zu allen Klassen, wobei die Vertrauenswahrscheinlichkeit für jede Klasse angegeben wird.</li>
                    <li><strong>Gradientenfreundlichkeit:</strong> Softmax ist differenzierbar, was für das Training von neuronalen Netzen mittels Backpropagation unerlässlich ist.</li>
                </ul>
            </div>

            <div class="border-t pt-8">
                <h2 class="text-2xl font-bold mb-4">Anwendung in LLMs</h2>
                <p>In Large Language Models (LLMs) wird die Softmax-Funktion typischerweise in der letzten Schicht für Aufgaben wie Textklassifikation, maschinelle Übersetzung oder Textgenerierung verwendet. Besonders bei der Sprachmodellierung spielt sie eine entscheidende Rolle:</p>
                <ul class="list-disc list-inside mt-4 pl-4 space-y-2">
                    <li><strong>Nächstes Wort Vorhersage:</strong> Nach der Verarbeitung einer Eingabesequenz generiert das LLM einen Vektor von Logits (unnormalisierte Scores) für jedes mögliche nächste Wort im Vokabular. Die Softmax-Funktion wandelt diese Logits in eine Wahrscheinlichkeitsverteilung über das gesamte Vokabular um. Das Wort mit der höchsten Wahrscheinlichkeit wird dann oft als das nächste Wort ausgewählt.</li>
                    <li><strong>Klassifikation:</strong> Wenn ein LLM zur Klassifikation von Text (z.B. Sentiment-Analyse, Themenkategorisierung) eingesetzt wird, werden die Logits für jede Kategorie durch Softmax in Wahrscheinlichkeiten umgewandelt.</li>
                </ul>
                <p class="mt-4">Die Softmax-Funktion ermöglicht es LLMs, nicht nur eine einzelne Ausgabe zu produzieren, sondern auch ein Mass an Konfidenz für jede mögliche Ausgabe zu liefern, was für die Analyse und weitere Verarbeitung von entscheidender Bedeutung ist.</p>
            </div>
        </div>
    </main>
</body>
</html>