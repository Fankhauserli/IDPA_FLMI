<!doctype html>
<html lang="de">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>IDPA FLMI — Training & Backpropagation</title>
    <meta name="description" content="Erklärung des Trainings von LLMs" />
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="../../../assets/css/styles.css" />
</head>
<body class="antialiased bg-gray-50 text-gray-800">
    <header class="bg-white shadow">
        <div class="max-w-6xl mx-auto px-4 sm:px-6 py-6">
            <a href="/" class="text-indigo-600 hover:underline">← Zurück zur Übersicht</a>
            <h1 class="text-3xl font-bold mt-2">Training & Backpropagation</h1>
            <p class="text-gray-600 mt-1">Wie ein neuronales Netzwerk lernt</p>
        </div>
    </header>
    <main class="max-w-4xl mx-auto px-4 py-12">
        <div class="bg-white p-8 rounded-lg shadow space-y-8">

            <div>
                <p class="text-lg leading-relaxed">Wenn man bei einem neuronalen Netzwerk «lernt» sagt, meint man damit eigentlich nichts anderes, als die Parameter zu ändern, sodass man ein gewünschtes Ergebnis bekommt. Dies ist jedoch nicht trivial, denn heutige LLMs haben über eine Billion Parameter. Man kann sich diese wie viele Schalter vorstellen, die man beliebig drehen kann, um einen anderen Wert zu erhalten.</p>
            </div>

            <div class="border-t pt-8">
                <h2 class="text-2xl font-bold mb-4">Backpropagation: Lernen aus Fehlern</h2>
                <p>Backpropagation ist der zentrale Algorithmus, mit dem ein neuronales Netzwerk lernt. Vereinfacht funktioniert der Algorithmus so, dass man einen Satz hat, den man kennt.</p>
                <p class="mt-4">Als Beispiel nehmen wir: «Ein Kirschbaum ist pink.» Jetzt füttern wir das Netzwerk mit diesem Satz, lassen jedoch das letzte Wort weg. Wir fragen das Netzwerk also, was das nächste Wort im Satz «ein Kirschbaum ist» ist.</p>
                <p class="mt-4">Das Netzwerk wird jetzt durch die Schritte gehen (Embedding, Attention, MLP) und eine Vorhersage machen, die wie folgt aussehen könnte:</p>
                <ul class="list-disc list-inside mt-4 pl-4 space-y-1">
                    <li>45% pink</li>
                    <li>20% gross</li>
                    <li>20% wunderschön</li>
                    <li>15% weiss</li>
                </ul>
                <p class="mt-4">Das Netzwerk hat «pink» mit 45% als wahrscheinlichstes nächstes Wort gewählt – Das ist richtig, jedoch mit 45% nicht überzeugend. Der <strong>Verlust</strong> (Loss) misst nun, wie weit die Vorhersage von der Wahrheit entfernt ist. Mathematisch nutzen wir dafür die Cross-Entropy: Je höher die vorhergesagte Wahrscheinlichkeit für das richtige Wort, desto kleiner der Verlust. In unserem Fall ist der Verlust 55%.</p>
                <div class="mt-4 border-2 border-dashed border-gray-300 p-6 text-center text-gray-500 rounded-lg">
                    <p class="font-semibold">Placeholder: Bild</p>
                    <p class="text-sm">Eine Tabelle, die die Vorhersage (45% pink) gegen das Ziel (100% pink) stellt und den daraus resultierenden Loss visualisiert.</p>
                </div>
                <p class="mt-4 font-semibold text-center text-xl">Das Ziel: Diesen Verlust minimieren.</p>
            </div>

            <div class="border-t pt-8">
                <h2 class="text-2xl font-bold mb-4">Lernen im Detail: Die Kettenregel</h2>
                <p>Doch wie funktioniert das Lernen im Detail? Stell dir ein winziges neuronales Netzwerk vor mit nur drei Schichten: zehn Input-Neuronen, vier Neuronen in einer versteckten Schicht und zwei Output-Neuronen.</p>
                <div class="my-4 border-2 border-dashed border-gray-300 p-6 text-center text-gray-500 rounded-lg">
                    <p class="font-semibold">Placeholder: Bild</p>
                    <p class="text-sm">Ein kleines neuronales Netzwerk.</p>
                </div>
                <p>Wir trainieren es damit, das Wort «Katze» als «Säugetier» zu klassifizieren. Im <strong>Forward-Pass</strong> fliesst die Aktivierung durch das Netz, am Ende kommt bei «Säugetier» ein Wert von 0,7 heraus – richtig, aber unsicher. Der Verlust beträgt -log(0,7) = 0,15.</p>
                <p class="mt-4">Jetzt beginnt der <strong>Backward-Pass</strong>: Dieser Fehler von 0,15 wandert rückwärts durch jede einzelne Verbindung. Für jedes Gewicht wird berechnet, wie stark es zum Fehler beigetragen hat.</p>
                <p class="mt-4">Die <strong>Kettenregel</strong> ist das mathematische Rückgrat des Backward-Passes. Sie erlaubt es, den Einfluss jedes Gewichts auf den Gesamtfehler effizient zu berechnen, indem die Ableitungen Schicht für Schicht rückwärts multipliziert werden.</p>
                <div class="my-4 border-2 border-dashed border-gray-300 p-6 text-center text-gray-500 rounded-lg">
                    <p class="font-semibold">Placeholder: Bild</p>
                    <p class="text-sm">Netzwerk mit roten Pfeilen (Backward-Pass) und Gradienten.</p>
                </div>
                <p>Die berechneten Gradienten zeigen die Richtung des steilsten Anstiegs des Fehlers. Wir wollen den Fehler aber minimieren, also gehen wir in die entgegengesetzte Richtung. Die <strong>Lernrate</strong> (eine kleine Zahl wie 0,01) bestimmt die Schrittgrösse:</p>
                <div class="my-4 p-4 bg-gray-800 text-white rounded-md">
                    <code class="font-mono">Neues Gewicht = altes Gewicht - Lernrate * Gradient</code>
                </div>
                <p>Ein Gewicht, das den Fehler vergrössert hat, wird verringert; eines, das ihn verringern würde, wird erhöht. Dies passiert für alle Milliarden Parameter gleichzeitig.</p>
            </div>

            <div class="border-t pt-8">
                <h2 class="text-2xl font-bold mb-4">Vom kleinen Netz zum LLM</h2>
                <p>Dieses Prinzip ist exakt das, was in LLMs passiert. Statt zehn Gewichten hat GPT-3 175 Milliarden. Statt drei Schichten hat es 96 Transformer-Blöcke. Der Ablauf bleibt identisch: Forward-Pass, Verlust berechnen, Backward-Pass und Parameter-Update.</p>
                <div class="my-4 p-4 bg-blue-50 border-l-4 border-blue-400 text-blue-800">
                    <p class="font-semibold">Anmerkung des Autors:</p>
                    <p class="text-sm"><em>Alle englischen Begriffe ins Glossar?</em></p>
                </div>
                <p>Was bei LLMs jedoch besonders ist: Die Embedding-Matrix selbst lernt mit. Während des Trainings rücken «Kirschbaum» und «Baum» im Vektorraum näher zusammen. Die Attention-Parameter lernen, welche Wörter im Satz wichtig sind. Dieser Zyklus wird milliardenfach wiederholt, bis aus dem 45%-Vorschlag für «pink» eine 99%-Wahrscheinlichkeit wird.</p>
                <p class="mt-4 text-lg leading-relaxed">Backpropagation ist also kein magischer Lernprozess, sondern ein mathematischer Fehlerkorrekturmechanismus, der durch die Kettenregel der Analysis effizient Millionen von Parametern gleichzeitig steuert. Ob zehn Gewichte oder 100 Milliarden – das Prinzip bleibt: <strong>Fehler messen, ableiten, anpassen und wiederholen.</strong></p>
            </div>
        </div>
    </main>
</body>
</html>