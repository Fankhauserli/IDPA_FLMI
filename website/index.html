<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDPA FLMI</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/responsive.css">
    <style>
                        /* Slider Container */
                        .slider-container {
                            position: relative;
                            overflow: hidden;
                            width: 100%;
                            border-radius: 8px;
                        }        
                /* Slider Track */
                .answers-slider {
                    display: flex;
                    transition: transform 0.3s ease-in-out;
                }
        
                /* Individual Slide */
                .answer {
                    flex: 0 0 100%;
                    width: 100%;
                    box-sizing: border-box;
                    padding: 20px;
                    background-color: #f9f9f9;
                    border-radius: 8px;
                }
        
                .dark-section .answer {
                    background-color: #333;
                }
        
                .answer h4 {
                    margin-top: 0;
                }
        
        
        /* Dots Navigation */
        .slider-dots {
            position: absolute;
            bottom: 10px; /* Position from the bottom */
            left: 50%; /* Start from the middle */
            transform: translateX(-50%); /* Shift back by half its width to truly center */
            display: flex; /* Use flexbox for spacing dots */
            justify-content: center; /* Center dots within the flex container */
            gap: 8px; /* Space between dots */
            z-index: 10; /* Ensure dots are above other content */
        }
        
        .slider-dots .dot {
            cursor: pointer;
            height: 12px;
            width: 12px;
            background-color: #bbb;
            border-radius: 50%;
            display: inline-block;
            transition: background-color 0.3s ease;
        }

        .slider-dots .dot.active {
            background-color: #717171;
        }

        /* On larger screens, show answers side-by-side */
        @media (min-width: 768px) {
            .answers-slider {
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 20px;
                transform: none !important; /* Disable sliding transform */
            }

            .answer {
                flex-basis: auto; /* Reset flex-basis */
            }

            .slider-dots {
                display: none; /* Hide dots on desktop */
            }
        }
    </style>
</head>
<body>
    <div id="header-placeholder"></div>

    <main>
        <section id="home" class="hero">
            <h1 class="gradient-text">Erforschung von Fehlinformationen in LLMs</h1>
            <h2>Interdisziplin√§re Projektarbeit</h2>
            <p>Entdecken Sie, wie Large Language Models Fehlinformationen verbreiten und wie wir dem entgegenwirken k√∂nnen.</p>
            <a href="#einleitung" class="button">Tauchen Sie ein</a>
        </section>

        <section id="einleitung">
            <h2 class="gradient-text">Einleitung</h2>
            <p>In der heutigen digitalen Landschaft sind Large Language Models (LLMs) √ºberall pr√§sent. Sie bilden das Herzst√ºck fast aller modernen Chatbots. Diese Systeme beeindrucken durch ihre F√§higkeit, komplexe Texte zu generieren, die oft nicht mehr von menschlichen Formulierungen zu unterscheiden sind. Doch hinter der sprachlichen Eleganz verbirgt sich eine fundamentale Herausforderung: Die Modelle sind darauf trainiert, einen sachlichen und selbstbewussten Tonfall zu pflegen, unabh√§ngig davon, ob der Inhalt faktisch korrekt ist. Dies f√ºhrt zur sogenannten ¬´Autorit√§ts-Falle¬ª, bei der die Nutzer den plausibel klingenden, aber fehlerhaften Antworten, den sogenannten Halluzinationen, blind vertrauen.</p>
            <p>Da die Modelle lediglich berechnen, was statistisch ¬´plausibel¬ª klingt, stellt sich die Frage, ob wir jemals eine KI ohne Fehlinformationen erleben werden. Diese Arbeit setzt genau hier an und untersucht die tieferliegenden Mechanismen von der Vektoreinbettung bis hin zum Training durch Backpropagation.</p>
            <p>Dazu analysieren wir nicht nur die Fehlerquellen in der Architektur, wie etwa das ¬´Snowballing¬ª in der Attention-Schicht, sondern bewerten auch moderne L√∂sungsans√§tze wie Retrieval Augmented Generation (RAG), das Anpassen der Temperatur sowie strategisches Prompt Engineering und Agent-Architekturen. Ziel ist es, ein Verst√§ndnis daf√ºr zu schaffen, warum KI ¬´l√ºgt¬ª und wie wir diese Systeme dennoch als wertvolle Werkzeuge nutzen k√∂nnen, ohne der ¬´Autorit√§ts-Falle¬ª zu erliegen.</p>
        </section>

        <section id="fragestellung" class="dark-section">
            <h2>Fragestellung</h2>
            <p>Im Zentrum steht die Untersuchung unserer Leitfrage. Diese haben wir w√§hrend der Arbeit angepasst, um noch gezielter ein Berech abzudecken. Folgendes ist unsere finale Fragestellung:</p>
            <h3>Inwiefern f√ºhren die mathematischen Grundlagen von Large Language Models zwangsl√§ufig zu Halluzinationen und wie wirksam sind aktuelle technische Gegenmassnahmen?</h3>
        </section>

        <section id="themen">
            <h2 class="gradient-text">√úbersicht</h2>
            <p>Um unsere Arbeit √ºbersichtlicher zu gestalten haben wir sie in folgende Bereiche unterteilt.</p>
            <div class="topic-links-container">
                <a href="/llm/" class="topic-link">
                    <div class="topic-box">
                        <h3>LLM</h3>
                        <p>Lernen Sie wie ein LLM funktioniert</p>
                        <span class="learn-more">Erfahre mehr</span>
                    </div>
                </a>
                <a href="/vorschlaege/" class="topic-link">
                    <div class="topic-box topic-box-special">
                        <h3>Unsere Vorschl√§ge</h3>
                        <p>Was k√∂nnen <strong>Sie</strong> gegen Fehlinformationen machen?</p>
                        <span class="learn-more">Erfahre mehr</span>
                    </div>
                </a>
                <a href="/anfaelligkeit/" class="topic-link">
                    <div class="topic-box">
                        <h3>Anf√§lligkeit</h3>
                        <p>Lenren Sie wie Fehler in LLM entstehen</p>
                        <span class="learn-more">Erfahre mehr</span>
                    </div>
                </a>
                <a href="#interview" class="topic-link">
                    <div class="topic-box">
                        <h3>Interview</h3>
                        <p>Interview mit Experten im LLM Bereich</p>
                        <span class="learn-more">Erfahre mehr</span>
                    </div>
                </a>
                <a href="#fazit" class="topic-link">
                    <div class="topic-box">
                        <h3>Fazit</h3>
                        <p>Unser Zusammenfassung der Erkenntnisse der IDPA</p>
                        <span class="learn-more">Erfahre mehr</span>
                    </div>
                </a>
            </div>
        </section>

        <section id="interview" class="tips">
            <h2 class="gradient-text">Interview</h2>
            <p>Um unsere Recherche mit Fachwissen abzust√ºrzen haben wir verschiedene Fachpersonen nach einem Interview gefragt. Urspr√ºnglich war die Idee jemand aus der ETH Z√ºrich zu befragen, da dort mit "Apertus" vor k√ºrzerem ein eigenes LLM entwickelt wurde. Leider antwortete uns niemand auf unsere Anfrage zu einem Interview.</p>
            <p>Auf der Webseite einer Fachhochschule wurden wir anschliessend auf eine Person, welche im Bereich LLM t√§tig ist, aufmerksam. Wir beschlossen diese anzuschreiben und nach einem Interview zu bitten. Nach einer initialen Zusage zu einem Interview, waren wir etwas entt√§uscht, als uns mitgeteilt wurde wir sollen unsere Interview-Fragen mit ChatGPT oder Perprexity erarbeiten. Im folgendem Abschnitt ist unser Mailverkehr mit der Person abgebildet. (Der Namen und T√§tigkeitsbereich der Person, wie auch der Fachhochschulname wurde geschw√§rzt, da wir keine Erlaubnis haben diesen zu publizieren)</p>
            <div class="tip-box">
                <div class="chat-container">
                    <h5 class="chat-header">Mailverkehr mit der genannten Person:</h5>
                    <div class="message message-right">
                        <details class="collapsible-message">
                            <summary><strong>Wir:</strong>&nbspAnfrage f√ºr ein Interview</summary>
                            <div class="collapsible-content-wrapper">
                                <div class="collapsible-content">
                                    <p>Sehr geehrter X</p>
                                    <p>Mein Name ist Fabio Fl√ºckiger und ich schreibe Ihnen im Namen meines Lernendenteams¬†von der GIBB (Gewerblich Industrielle Berufsfachschule Bern), Berufsmaturit√§t. Im Rahmen unseres interdisziplin√§ren Projekts (IDPA) untersuchen wir das Verhalten¬†von Fehlinformationen innerhalb von k√ºnstlicher Intelligenz, insbesondere grosser Sprachmodelle (LLMs).</p>
                                    <p>Unsere zentrale Fragestellung lautet: "Wie kann bei K√ºnstlichen Intelligenzen¬†die Auswirkung von Falschinformationen technisch eingeschr√§nkt werden?‚Äú</p>
                                    <p>F√ºr den methodischen Teil unserer Arbeit, in dem wir die Struktur moderner Algorithmen und die Urspr√ºnge von Fehlinformationen analysieren, w√§re ein Interview mit einer Fachperson von unsch√§tzbarem Wert.¬†</p>
                                    <p>Wir sind durch die offizielle <span class="censored">Webseite der FH</span> auf <span class="censored">den T√§tigkeitsbereich der Person</span> aufmerksam geworden, bei welcher¬†Sie hinterlegt sind.</p>
                                    <p>H√§tten Sie Zeit f√ºr ein Interview? Wir k√∂nnen Ihnen die Fragen schriftlich zukommen lassen oder nach Terminvereinbarung das Interview per Videokonferenz, Telefon oder pers√∂nlich durchf√ºhren.</p>
                                    <p>Vielen Dank f√ºr Ihre Zeit und Ihre Aufmerksamkeit.</p>
                                    <p>Freundliche Gr√ºsse<br>
                                    Isaiah Dias<br>
                                    Linus Fankhauser<br>
                                    Mihailo Milosevic<br>
                                    Fabio Fl√ºckiger
                                    </p>
                                </div>
                            </div>
                        </details>
                    </div>
                    <div class="message message-left">
                        <details class="collapsible-message">
                            <summary><strong>X:</strong>&nbspZusage schriftliches Interview</summary>
                            <div class="collapsible-content-wrapper">
                                <div class="collapsible-content">
                                    <p>Lieber Fabio Fl√ºckiger</p>
                                    <p>Ihr k√∂nnt mir gerne eure Fragen schriftlich schicken, ich kann sie beantworten oder an eine entsprechende Fachperson weiterleiten.</p>
                                    <p>Viel Erfolg mit der Arbeit</p>
                                    <p>Mit freundlichem Gruss</br>
                                    X</p>
                                </div>
                            </div>
                        </details>
                    </div>
                    <div class="message message-right">
                        <details class="collapsible-message">
                            <summary><strong>Wir:</strong>&nbspInterviewfragen</summary>
                            <div class="collapsible-content-wrapper">
                                <div class="collapsible-content">
                                    <p>Guten Tag Herr X</p>
                                    <p>Ihre Antwort freut uns enorm. Vielen Dank.</p>
                                    <p>Es w√ºrde uns freuen, wenn Sie folgende Fragen beantworten k√∂nnten. Wir haben uns schon etwas l√§nger mit dem Thema LLM‚Äôs auseinandergesetzt und schliessen alle dieses Jahr die Lehre als Informatiker EFZ ab. Somit bitten wir Sie, die Fragen m√∂glichst detailiert zu beantworten.</p>
                                    <ul>
                                        <li>Warum treten Halluzinationen auf? Was sind die Problemstellen innerhalb einer LLM / eines Transformers?</li>
                                        <li>Ist es m√∂glich, ein RAG-System aufzubauen, welches die Informationen direkt aus dem Internet bezieht? (somit verl√§sst man sich nicht nur auf die Trainingsdaten ‚Äì kann dies ein L√∂sungsansatz gegen Halluzinationen sein?)</li>
                                        <li>Wird das Problem von Fehlinformationen durch das Skalieren der LLM behoben?</li>
                                        <li>Welche Entwicklungen hatten bis jetzt den gr√∂ssten Einfluss auf die Fehlerquote?</li>
                                        <li>Welche Massnahmen werden aktuell entwickelt oder erforscht, um Halluzinationen weiter zu verhindern?</li>
                                        <li>Was k√∂nnen private Anwender von ChatBots machen, um Halluzinationen zu verhindern?</li>
                                        <li>Wie gross sch√§tzen Sie die Auswirkung von gutem Prompt-Engineering?</li>
                                        <li>Bringt es einen messbaren Mehrwert, wenn das Modell seine eigene Antwort in einem zweiten Schritt √ºberpr√ºft?</li>
                                        <li>Denken Sie, dass wir bei der ¬´Next-Token-Prediction¬ª an ein Limit stossen werden?</li>
                                        <li>Sollten wir unsere Zeit in die Forschung f√ºr neue Algorithmen investieren, welche einen anderen Ansatz verfolgen?</li>
                                    </ul>
                                    <p>Damit wir Sie kurz in unserer Arbeit vorstellen k√∂nnen, w√ºrden wir uns zus√§tzlich √ºber einen kurzen Steckbrief freuen. Besonders die Frage inwiefern Sie mit LLM‚Äôs zu tun haben.</p>
                                    <p>Wir danken Ihnen f√ºr Ihre Bereitschaft und sind gespannt auf Ihre Antworten.</p>
                                    <p>
                                        Freundliche Gr√ºsse<br>
                                        Mihailo Milosevic<br>
                                        Isaiah Dias<br>
                                        Linus Fankhauser<br>
                                        Fabio Fl√ºckiger
                                    </p>
                                </div>
                            </div>
                        </details>
                    </div>
                     <div class="message message-left">
                        <details class="collapsible-message">
                            <summary><strong>X:</strong>&nbspL√∂sungsansatz</summary>
                            <div class="collapsible-content-wrapper">
                                <div class="collapsible-content">
                                    <p>Lieber Fabio</p>
                                    <p>Danke f√ºr deine Fragestellungen. Diese Fragen sind sinnvoll und gut formuliert. Sie sind aber auch knifflig und haben nicht unbedingt eindeutige Antworten.</p>
                                    <p>Die Beantwortung dieser Fragen ist aber eigentlich der Inhalt eurer Projektarbeit und nicht meine Aufgabe, sondern eure üòä.</p>
                                    <p>Ich schlage euch vor, ihr sucht nach ersten Antworten mit ChatGPT oder Copilot usw, und auch nach einer Antwort f√ºr das Gesamt Thema eurer Semesterarbeit √† danach jede Frage vielleicht pr√§ziser stellen und nach Beispielen fragen.</p>
                                    <p>Ebenfalls k√∂nnt ihr nach B√ºchern oder Publikationen suchen eurem Thema: ein guter Ort ist das Literatursuchprogramm elicit.com, oder perplexity.ai</p>
                                    <p><strong>Am Schluss erstellt ihr eine Liste mit den 5-10 wichtigsten Faktoren, welche die Projektfrage beantworten "Wie kann bei K√ºnstlichen Intelligenzen¬†die Auswirkung von Falschinformationen technisch eingeschr√§nkt werden?‚Äú</strong></p>
                                    <p><strong>Zu jedem Faktor Hinweise, was zu tun ist, und ein Beispiel.</strong></p>
                                    <p>Gerne hier meine erste Anfrage bei Perplexity ¬†( gute √úbersicht meines Erachtens)</p>
                                    <p><a href="https://www.perplexity.ai/search/wie-kann-bei-kunstlichen-intel-3EBYzBdbSuS1x.QWbQjjvg#0" target="_blank">https://www.perplexity.ai/search/wie-kann-bei-kunstlichen-intel-3EBYzBdbSuS1x.QWbQjjvg#0</a></p>
                                    <p>und ChatGPT</p>
                                    <p><a href="https://chatgpt.com/share/69678a57-489c-8006-8b25-d7c0fd8f4eb2" target="_blank">https://chatgpt.com/share/69678a57-489c-8006-8b25-d7c0fd8f4eb2</a></p>
                                    <p>Mit freundlichem Gruss</p>
                                    <p>X</p>
                                </div>
                            </div>
                        </details>
                    </div>
                </div>
            </div>
            <p>Wir wissen nicht, warum die Person die Fragen nicht selbst√§ndig beantwortet hat. Vielleicht gab es ein Missverst√§ndnis und er dachte er w√ºrde durch das Beantworten der Frage unsere Recherche √ºbernehmen. Wobei wir viel mehr seine pers√∂nliche Meinung erfahren wollten. M√∂glicherweise waren unsere Frage auch zu simpel und er wollte uns Auffordern, uns noch tiefer ins Thema einzulesen. Eine weitere Option besteht darin, dass wir mit unserer Recherche in ein Gebiet vorgestossen sind, in welchem es selbst f√ºr Fachpersonen schwer ist Aussagen zu geben. Entsprechend seiner Empfehlung haben wir die Interview-Fragen mit einer k√ºnstlichen Intelligenz erarbeitet.</p>
            <p>Nun haben wir Fragen, √ºber Fehlinformationen und Halluzinationen innerhalb k√ºnstlicher Intelligenz, mit einer k√ºnstlichen Intelligenz beantworten lassen. Aufgrund dieser Ironie haben wir uns entschlossen weiter nach einem m√∂glichen Interview Ausschau zu halten. Im erweiterten Bekanntenkreis stiessen wir auf Mikaela, gelehrte Informatikerin Applikationsentwicklung EFZ und im letzten Semester des Studienganges "BSc Data Science & Artificial Intelligence" an der Fachhochschule Nordwestschweiz (FHNW). Neben dem Studium arbeitet Mikaela aktuell an einem generativem KI Projekt, welches mit Transformer-√§hnlichen Strukturen (TRELLIS) arbeitet. Das Ziel dieses Projektes ist es 3D Rekonstruktion aus 2D Bildern zu erstellen. Weitere Informationen √ºber ihr Werdegang k√∂nnen in ihrem <a href="https://dpoportfolio-mikaela.onrender.com/" target="_blank">Portfolio</a> gefunden werden. Sie stimmte unserem Interview zu und beantwortet unsere Fragen schriftlich.</p>
                    <div class="question-block">
                        <h5 class="chat-header">Warum treten Halluzinationen auf?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Halluzinationen entstehen prim√§r dadurch, dass Transformer als reine Vorhersagesysteme f√ºr das n√§chste Textelement konzipiert sind und √ºber kein internes Verst√§ndnis von Wahrheit oder Fakten verf√ºgen. Da diese Modelle darauf trainiert werden, menschliche Sprache und Schreibstile zu imitieren, priorisieren sie oft die Plausibilit√§t und den sprachlichen Fluss gegen√ºber der faktischen Korrektheit. Problemstellen liegen insbesondere in Wissensl√ºcken innerhalb der Trainingsdaten sowie in einer mangelhaften Kalibrierung, die dazu f√ºhrt, dass das Modell bei Unsicherheit oder fehlender Evidenz dennoch eine Antwort generiert. Zus√§tzliche Fehlerquellen ergeben sich durch Abweichungen zwischen dem Trainingszeitpunkt und der aktuellen Anfrage sowie durch Ungenauigkeiten im Abrufprozess externer Informationen, was insgesamt zu einer Diskrepanz zwischen statistischer Wahrscheinlichkeit und realer Faktenlage f√ºhrt.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/16e1eedaea84" target="_blank">https://gemini.google.com/share/16e1eedaea84</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Halluzinationen k√∂nnen aus vielen Quellen entstehen, beispielsweise durch Overfitting oder Underfitting, je nachdem, wie die Trainingsdaten und/oder die Struktur des Transformers/LLMs sind oder wie einige Prozesse w√§hrend der Datenaufbereitung stattfanden. Ein leicht illustrierbares Beispiel, das in letzter Zeit viel Aufmerksamkeit erhalten hat, ist die Frage ‚ÄûIs there a seahorse emoji?‚Äù bei ChatGPT. Da ChatGPT auf Next-Word-Prediction trainiert wurde und kein Seahorse-Emoji kennt, fiel es aufgrund interner Gewichte in einen langen Loop, in dem es sich dachte: ‚ÄûJa, ich kenne das Seahorse-Emoji‚Äù, ein falsches Emoji postete, sich dann entschuldigte und wieder von vorne begann.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Ist es m√∂glich, ein RAG-System aufzubauen, welches die Informationen direkt aus dem Internet bezieht?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Technisch ist dies absolut m√∂glich, indem man Sprachmodelle mit einer Live-Suche kombiniert und die gefundenen Dokumente direkt als Kontext f√ºr die Antwortgenerierung nutzt. Dieses Verfahren hilft oft effektiv gegen Halluzinationen, da die Aussagen auf aktuell recherchierten und verifizierten Fakten basieren statt rein auf dem Modellwissen. Der Erfolg h√§ngt jedoch massgeblich von der Qualit√§t der Quellen und der Abrufmechanik ab, da ungenaue Suchergebnisse sonst lediglich zu neuen Fehlern f√ºhren w√ºrden.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/473dfcb65adb" target="_blank">https://gemini.google.com/share/473dfcb65adb</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Ja, es w√§re m√∂glich, aber es ist eine schreckliche Idee, die die ‚ÄûHalluzinationen‚Äù nur verschlimmert. Wenn wir Live-Daten aus dem Internet beziehen, beziehen wir unglaublich viele unverifizierte Informationen, oft auch Informationen, die sich direkt widersprechen. Wenn wir wahllos Daten aus dem Internet beziehen, f√ºgen wir unserem Datensatz nur mehr Rauschen hinzu. Und auch wenn diese Daten eine Ahnung von aktuellen Themen haben, sind sie wahrscheinlich inkorrekt und somit ein grosses Risiko f√ºr Firmen, die LLMs anbieten. Wenn wir rein hypothetisch ein System aufbauen, das Daten automatisch korrekt und gut recherchiert bezieht, w√§re dies m√∂glich und k√∂nnte Halluzinationen verringern. Das rigorose √úberpr√ºfen der aktuell schon vorhandenen Daten w√§re jedoch ein gleichwertiger, wenn nicht sogar besserer Ansatz, um Halluzinationen zu verhindern. Oft sind aktuelle Themen auch noch entwickelnde Geschichten bei welchen Halbwahrheiten und Rumore Vortritt vor Fakten nehmen.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Wird das Problem von Fehlinformationen durch das Skalieren der LLM behoben?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Das reine Skalieren von Modellen behebt das Problem der Fehlinformation nicht zuverl√§ssig. Zwar verbessern sich mit zunehmender Gr√∂sse viele F√§higkeiten und die Fehlerrate sinkt, doch die Wahrhaftigkeit steigt dabei nicht automatisch linear an. Gr√∂ssere Modelle sind folglich nicht zwingend ehrlicher, weshalb gezielte Anpassungen wie Instruction-Tuning oft entscheidender sind als reine Modellgr√∂sse.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/41a74bcf8c8e" target="_blank">https://gemini.google.com/share/41a74bcf8c8e</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Das Problem l√§sst sich durch Skalierung und mehr Informationen reduzieren. Mehr Verst√§ndnis dieser Informationen f√ºhrt nat√ºrlich zu einer Verminderung solcher Fehlinformationen. Eine vollst√§ndige Behebung wird jedoch nie m√∂glich sein, solange Fehlinformationen in der Welt existieren ‚Äì was sie vorhersehbar immer tun werden.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Welche Entwicklungen hatten bis jetzt den gr√∂ssten Einfluss auf die Fehlerquote?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Den gr√∂ssten Einfluss hatten effizientere Trainingsmethoden zur Balance von Daten und Rechenleistung sowie die gezielte Ausrichtung der Modelle durch menschliches Feedback. Erg√§nzend dazu reduzierten der Zugriff auf externe Quellen zur Faktenpr√ºfung und spezielle Eingabetechniken f√ºr logische Schlussfolgerungen die Fehlerraten erheblich, wobei erst pr√§zisere Messverfahren eine solch gezielte Optimierung der Wahrheitstreue m√∂glich machten.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/f1577ac31a56" target="_blank">https://gemini.google.com/share/f1577ac31a56</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Es gibt viele verschiedene Ans√§tze, die der Entwicklung von LLMs helfen k√∂nnen. Human Feedback ist immer ein starker Kandidat f√ºr Verbesserungen. Auch generelle Erl√§uterungen bei schwierigen Themen, bei denen sich KI nicht sicher ist und dies auch klarmacht, sind hilfreich. Jedoch bin ich nicht intim besch√§ftigt mit der Historie der Entwicklung von Large Language Modellen, es steht sicher noch mehr Online dazu als ich hier vermittle.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Welche Massnahmen werden aktuell entwickelt oder erforscht, um Halluzinationen weiter zu verhindern?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Die Forschung konzentriert sich aktuell stark auf die Weiterentwicklung von Retrieval-Augmented Generation, indem iterative Korrekturprozesse und dynamische Suchverfahren integriert werden, die Informationen erst bei explizitem Bedarf abrufen. Parallel dazu werden Mechanismen etabliert, die Modelle dazu zwingen, ihre eigenen Aussagen durch Verifikationsketten zu pr√ºfen, externe Werkzeuge zur inhaltlichen Entlastung zu nutzen oder Antworten bei Unsicherheit und fehlenden Belegen g√§nzlich zu verweigern.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/37a5f038ca2f" target="_blank">https://gemini.google.com/share/37a5f038ca2f</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Auch hier bin ich keine Expertin, mir ist jedoch bewusst, dass es seit einiger Zeit ein erh√∂htes Interesse an der Erkl√§rbarkeit von KI gibt, insbesondere in LLMs. Es geht darum, zu untersuchen, was genau in den verschiedenen Gewichten und Vorurteilen eines Layers abl√§uft und wie allein schon die Einbettungen auf einer intimen und intuitiven Ebene erkl√§rt werden k√∂nnen ist noch ein entwickelndes Thema, welches uns auch hilft exakte Massnahmen zu treffen um LLMs in diesen Ans√§tzen zu verbessern.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Was k√∂nnen private Anwender von ChatBots machen, um Halluzinationen zu verhindern?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Private Anwender k√∂nnen Halluzinationen effektiv entgegenwirken, indem sie konsequent Belege wie Zitate oder Links einfordern und bevorzugt Modi mit aktiver Internetrecherche nutzen. Zudem empfiehlt es sich, das Modell anzuweisen, bei Unsicherheit zu schweigen, sowie Antworten durch mehrfache Abfragen auf ihre Konsistenz zu pr√ºfen und komplexe Fragen in einzeln verifizierbare Fakten zu zerlegen.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/febe989dc02a" target="_blank">https://gemini.google.com/share/febe989dc02a</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Exaktes und einheitliches prompten, es gibt viele Arten wie man LLMs prompten kann um mehr Informationen zu gewinnen √ºber den Generierungsprozess. Ebenfalls haben LLMs einen limitierten Kontext, somit sollte man Vorsichtig sein viele Sachen auf einmal zu Fragen. Ich kann empfehlen oft nach Quellen von Antworten zu fragen und diese zu √ºberpr√ºfen und jeweils nur 1 Frage pro Nachricht zu senden.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Wie gross sch√§tzen Sie die Auswirkung von gutem Prompt-Engineering?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Die Auswirkung ist empirisch substanziell, jedoch stark von der jeweiligen Aufgabe abh√§ngig. Zwar k√∂nnen gezielte Strukturierung und Methoden wie Chain-of-Thought die Fehlerquoten massgeblich senken und das Reasoning verbessern, sie ersetzen jedoch kein fundiertes Grounding.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/f881278bfb1b" target="_blank">https://gemini.google.com/share/f881278bfb1b</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Prompt-Engineering kann dem Output massiv helfen das zu kriegen was man erwartet, mehr Kontext verhilft Sprachmodellen exaktere Quellen zu beziehen und Outputs nach diesen Quellen zu produzieren.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Bringt es einen messbaren Mehrwert, wenn das Modell seine eigene Antwort in einem zweiten Schritt √ºberpr√ºft?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Eine √úberpr√ºfung im zweiten Schritt bietet oft messbaren Mehrwert, indem sie durch iterative Feedbackschlaufen oder das Abw√§gen mehrerer L√∂sungswege zu robusteren Antworten f√ºhrt und Halluzinationen reduziert.</p>
                                    <p>Dieser Nutzen ist allerdings nicht garantiert, da die Zuverl√§ssigkeit der Selbstkorrektur stark vom Setup sowie dem jeweiligen Evaluationsmass abh√§ngt und negative Befunde zeigen, dass Modelle ihre Antworten nicht in jedem Kontext erfolgreich verbessern.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/bc31629f968c" target="_blank">https://gemini.google.com/share/bc31629f968c</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Nein, ein und dasselbe Modell w√ºrde nur seine eigenen Verzerrungen und Fehler verst√§rken. Auch wenn man ein anderes Modell n√ºtzt hilft man hier nur wenig, das kein Modell an sich eine Wahrheitsbasis besitzt.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Denken Sie, dass wir bei der ¬´Next-Token-Prediction¬ª an ein Limit stossen werden?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Zwar skaliert Next-Token-Prediction gem√§ss den bekannten Gesetzen erstaunlich weit und profitiert stetig von mehr Rechenleistung, doch stossen wir bei Anforderungen wie Wahrheitstreue, langfristiger Planung und Werkzeugnutzung an qualitative Grenzen. Da reine Vorhersagemodelle keine Faktenreue garantieren, liegt der Fortschritt nicht allein in der weiteren Skalierung, sondern in der Erg√§nzung durch Mechanismen wie Retrieval und Verifikation, um diese strukturellen L√ºcken zu schliessen.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/ea56ae772f1e" target="_blank">https://gemini.google.com/share/ea56ae772f1e</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Das h√§ngt davon ab, was hier mit dem Limit von genau gemeint ist. Nat√ºrlich ist eine Next-Token-Prediction nicht f√ºr Anwendungsf√§lle ausserhalb der Sprache optimal. Mit Transformer-Modellen, die viele verschiedene Architekturen nutzen, k√∂nnen wir jedoch Modelle bauen, die f√ºr verschiedene Aufgaben viel besser geeignet sind. Aktuell sind wir ja auch schon weiter als nur ‚ÄûNext-Token-Prediction‚Äú und moderne Architekturen arbeiten mit verschiedenen Tasks zur Optimierung.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <div class="question-block">
                        <h5 class="chat-header">Sollten wir unsere Zeit in die Forschung f√ºr neue Algorithmen investieren, welche einen anderen Ansatz verfolgen?</h5>
                        <div class="answers-container slider-container">
                            <div class="answers-slider">
                                <div class="answer">
                                    <h4>KI-generierte Antwort</h4>
                                    <p>Strategisch gesehen deutet die aktuelle Evidenz darauf hin, dass wir weniger in isolierte Alternativen, sondern in hybride Ans√§tze investieren sollten. Besonders erfolgversprechend ist die Kombination aus Generierung, Recherche und Verifikation sowie der Einsatz von Sprachmodellen zur Steuerung externer Werkzeuge, um durch systematisches Verst√§ndnis bestehende Forschungsl√ºcken effektiv zu schliessen.</p>
                                    <p style="font-size: 0.8em; color: #666; margin-top: 10px;">Quelle: <a href="https://gemini.google.com/share/b0340b616781" target="_blank">https://gemini.google.com/share/b0340b616781</a></p>
                                </div>
                                <div class="answer">
                                    <h4>Antwort von Mikaela</h4>
                                    <p>Ja, grunds√§tzlich schon. Es gibt noch sehr viel Arbeit im Umfeld von KI und insbesondere von Transformer-Modellen in allen m√∂glichen Richtungen: Performance, Rechenzeit, Erkl√§rbarkeit, Stromverbrauch usw. Wie schon zuvor sollten wir uns weiterhin bem√ºhen, neue Architekturen und Modellans√§tze zu entwickeln.</p>
                                </div>
                            </div>
                            <div class="slider-dots"></div>
                        </div>
                    </div>
                    <h4>Auswertung des Interviews</h4>
                    <p>Die Interviews mit Mikaela und der generativen AI decken unsere Recherche gut ab. Entsprechend unser Erwartung k√∂nnen sowohl Anwender, wie auch Entwickler auf Halluzinationen Einfluss nehmen. Klar erkenntlich ist, dass in diesem Feld bereits viel gemacht wurde aber auch weiterhin viel geforscht wird. Entsprechend unser Erwartung ist das Prompt Engineering f√ºr private Personen am wichtigsten. Bei der Entwicklung gibt es zwei unterschiedliche Bereiche in welchen in Zukunft weiterentwickelt werden kann. Der KI Antworten entnehmen wir, dass aktuell darauf konzentriert werden soll, mit verschiedenen Umsystemen oder weiteren Mechanismen Halluzinationen, in der Transformer Architektur, zu vermindern. Mikaela erw√§hnt auch diesen Ansatz, ausserdem teilte sie mit, dass wir auch weitere Architekturen und Modellans√§tze entwickeln sollten.</p>

                    <p>Zus√§tzlich ist uns beim Auswerten des Interviews noch etwas weiteres aufgefallen. Gemini antwortete, dass aktuell Mechanismen entwickelt werden, damit LLM Antworten, bei fehlenden Quellen oder Unsicherheiten, verweigern k√∂nnen. Mikaela hat bei zwei Fragen sich Distanziert und mit Aussagen wie "bin ich keine Expertin" klargemacht, dass die Fakten nicht vollst√§ndig sind oder sie etwas unsicher ist. Dies w√§re sicherlich eine der gr√∂ssten Entwicklungen gegen Halluzinationen. Wenn ein LLM Situationen erkennen, in welchen zu wenig Belege vorhanden sind, und diese anschliessend markieren h√§tten die Benutzer schon eine erste Einsch√§tzung wie wahrheitsgetreu die Antwort sein kann.</p>

                    <p>Ein grosses Erkenntnis aus dem Mailverkehr mit der anonymisierter Person und dem Interview war, dass unsere Fragestellung immer noch zu allgemein gehalten war. Daher passten wir diese noch w√§hrend der Arbeit an.</p>
                </section>
        <section id="fazit" class="gradient-section">
            <h2>Fazit</h2>
            <p>KI Chatbots beeinflussen schon heute wie wir an Informationen gelangen. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. KI Chatbots beeinflussen schon heute wie wir an Informationen gelangen. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
            <p>KI Chatbots beeinflussen schon heute wie wir an Informationen gelangen. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. KI Chatbots beeinflussen schon heute wie we an Informationen gelangen. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
        </section>
    </main>

    <div id="footer-placeholder"></div>

    <script src="/js/main.js"></script>
    <script src="/js/footer.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const allDetails = document.querySelectorAll('.collapsible-message');
            
            allDetails.forEach(details => {
                const summary = details.querySelector('summary');
                const contentWrapper = details.querySelector('.collapsible-content-wrapper');

                // Set initial max-height for animation
                if (!details.open) {
                    contentWrapper.style.maxHeight = '0px';
                }

                summary.addEventListener('click', (event) => {
                    event.preventDefault(); // Prevent the default instant toggle

                    if (details.open) {
                        // Start closing animation
                        contentWrapper.style.maxHeight = '0px';
                        // When the animation is done, remove the 'open' attribute
                        contentWrapper.addEventListener('transitionend', function handler() {
                            details.removeAttribute('open');
                            contentWrapper.removeEventListener('transitionend', handler);
                        });
                    } else {
                        // Set the 'open' attribute to start
                        details.setAttribute('open', '');
                        // Then set the max-height to trigger the opening animation
                        contentWrapper.style.maxHeight = contentWrapper.scrollHeight + 'px';
                    }
                });
            });
        });

        const sliders = document.querySelectorAll('.slider-container');

        sliders.forEach(slider => {
            const sliderTrack = slider.querySelector('.answers-slider');
            const slides = slider.querySelectorAll('.answer');

            const dotsContainer = slider.querySelector('.slider-dots');
            
            if (!sliderTrack || slides.length === 0) {
                return;
            }

            let currentIndex = 0;
            let touchStartX = 0;
            let touchEndX = 0;

            // Create dots
            slides.forEach((_, index) => {
                const dot = document.createElement('span');
                dot.classList.add('dot');
                if (index === 0) {
                    dot.classList.add('active');
                }
                dot.addEventListener('click', () => {
                    goToSlide(index);
                });
                dotsContainer.appendChild(dot);
            });

            const dots = dotsContainer.querySelectorAll('.dot');

            function goToSlide(index) {
                if (index < 0 || index >= slides.length) {
                    return;
                }
                currentIndex = index;
                const offset = -currentIndex * 100;
                sliderTrack.style.transform = `translateX(${offset}%)`;
                updateDots();
            }

            function updateDots() {
                dots.forEach((dot, index) => {
                    if (index === currentIndex) {
                        dot.classList.add('active');
                    } else {
                        dot.classList.remove('active');
                    }
                });
            }



            // Swipe functionality
            sliderTrack.addEventListener('touchstart', (e) => {
                touchStartX = e.changedTouches[0].screenX;
            });

            sliderTrack.addEventListener('touchend', (e) => {
                touchEndX = e.changedTouches[0].screenX;
                handleSwipe();
            });

            function handleSwipe() {
                if (window.innerWidth >= 768) return; // Only on mobile
                
                const swipeThreshold = 50; // Minimum swipe distance
                if (touchStartX - touchEndX > swipeThreshold) {
                    // Swiped left
                    const newIndex = (currentIndex + 1) % slides.length;
                    goToSlide(newIndex);
                } else if (touchEndX - touchStartX > swipeThreshold) {
                    // Swiped right
                    const newIndex = currentIndex - 1 < 0 ? slides.length - 1 : currentIndex - 1;
                    goToSlide(newIndex);
                }
            }
            
            // Initial setup
            goToSlide(0);
        });
    </script>
</body>
</html>
