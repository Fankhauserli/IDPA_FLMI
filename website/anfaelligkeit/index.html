<!DOCTYPE html>
<html lang="de">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDPA FLMI - Anfälligkeit</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/responsive.css">
</head>

<body>
    <div id="header-placeholder"></div>

    <main>
        <section class="hero">
            <h1 class="gradient-text">Anfälligkeit</h1>
            <p>Warum LLMs Fehler machen und was wir dagegen tun können.</p>
        </section>

        <!-- Intro Section -->
        <section id="einleitung">
            <h2 class="gradient-text">Einleitung</h2>
            <h3>Halluzinationen</h3>
            <p>Es lässt sich feststellen, dass Large Language Models (LLM), wie ChatGPT, noch nicht in vollem Umfang
                perfekt sind. Ein wesentlicher Kritikpunkt ist, dass diese Modelle häufig Falschinformationen
                erzeugen, die auf den ersten Blick glaubwürdig wirken, was in der Fachliteratur als Halluzination
                bezeichnet wird. Gemäss den Ergebnissen einer Studie der BBC manifestiert sich dieses Phänomen mit
                einer höheren Frequenz, als gemeinhin angenommen wird. Gemäss dieser Studie manifestieren sich in
                nahezu 45 % der Antworten, die von KI-Assistenten auf nachrichtenbezogene Fragen gegeben werden,
                mindestens eine signifikante Inkonsistenz. Die vorliegende Untersuchung, die in Kooperation mit der
                European Broadcasting Union (EBU) durchgeführt wurde, verdeutlicht, dass die technologische
                Zuverlässigkeit noch weit hinter dem öffentlichen Vertrauen zurückbleibt.</p>
            <p>Ein besonders problematischer Aspekt ist dabei die sogenannte Autoritäts-Falle: Die Modelle sind
                darauf trainiert, Informationen in einem sachlichen, selbstbewussten und seriösen Tonfall zu
                präsentieren. In der Konsequenz wird es für den Rezipienten nahezu unmöglich, allein durch das Lesen
                des Textes zu differenzieren, ob es sich um eine faktisch korrekte Antwort oder eine täuschend echt
                wirkende Halluzination handelt. In etwa 31 % der Fälle wurde seitens der Forscher zudem
                festgestellt, dass die von der KI offerierten Quellen und Zitate entweder in die Irre führten, auf
                nicht existente Webseiten verweisen oder den eigentlichen Inhalt der Antwort gar nicht stützten.</p>
            <p>Es ist von Interesse, dass die Fehlerquote signifikant zwischen den verschiedenen Anbietern variiert.
                Während ChatGPT und Microsoft Copilot in der Studie bei etwa einem Drittel der Antworten Mängel
                aufwiesen, schnitt Google Gemini mit einer Fehlerquote von 76 % bei den Quellenangaben deutlich
                schlechter ab. Dies veranschaulicht, dass die Integration von Echtzeit-Daten aus dem Internet nach
                wie vor eine signifikante technische Herausforderung darstellt, die bislang noch nicht bewältigt
                werden konnte. Ein weiteres kritisches Ergebnis der Studie ist das Sprachgefälle: Englischsprachige
                Anfragen weisen demnach eine hohe Bearbeitungsstabilität auf, während das Risiko einer
                Desinformation bei kleineren europäischen Sprachen signifikant ansteigt.</p>
            <p>Zusammenfassend äussert der Bericht die Besorgnis einer schleichenden Erosion des Vertrauens in
                Fakten. Die zunehmende Nutzung von KI-Systemen als primäre Informationsquelle birgt die Gefahr der
                unbewussten Verbreitung falscher Narrative, welche das Fundament des Qualitätsjournalismus
                untergraben können. Die Experten der EBU und der BBC kommen daher zu dem Schluss, dass
                KI-Assistenten zum jetzigen Zeitpunkt eher als kreative Werkzeuge und nicht als verlässliche
                Fakten-Bibliotheken betrachtet werden sollten. Eine manuelle Überprüfung der Primärquellen ist somit
                für jeden Nutzer unerlässlich, um nicht Opfer dieser als "plausibel klingend" zu bezeichnenden
                Falschinformationen zu werden.</p>
            </div>
        </section>

        <!-- Technical Causes -->
        <section id="ursachen">
            <h2 class="gradient-text">Ursachen</h2>
            <h3>Warum passieren Fehler?</h3>
            <p>Das Maschinen Fehler machen scheint auf den ersten Blick einfach lösbar, denn eine Maschine macht ja
                nur das, was man ihr sagt, also kann man einer LLM nicht einfach sagen, dass sie keine Fehler machen
                soll? Die Antwort ist komplex, denn im Gegensatz zu herkömmlicher Software basieren LLMs nicht auf
                logischen Regeln, sondern auf statistischen Wahrscheinlichkeiten. Sie “wissen” nichts, sie berechnen
                lediglich die plausibelste Fortsetzung eines Textes.</p>
            <p>Wie im Teil <a href="/llm/index.html#training-detail">Training</a> beschrieben ist das fundamentale
                Problem das
                Trainingsziel:
                Next-Token-Prediction. Das Modell wird darauf trainiert, den statistischen Fehler (Cross-Entropy
                Loss) bei der Vorhersage des nächsten Wortes zu minimieren, nicht den faktischen Fehler.</p>

            <h4>Fehler im Embedding</h4>
            <p>Wie im <a href="/llm/index.html#vektoren">Embedding Teil</a> beschrieben werden Wörter, die in ähnlichen
                Kontexten
                vorkommen, im Vektorraum
                sehr nahe beieinander dargestellt. So zum Beispiel sind die Vektoren, welche die Wörter Airbus und
                Boeing darstellen sehr ähnlich. Wenn ein Modell jetzt halluziniert, greift es nicht komplett
                daneben, sondern nur ein bisschen, also kann zum Beispiel bei dem Satz: “Das Flugzeug A350 wird
                hergestellt von” das nächste Wort in der Vektoriellen Darstellung näher bei Boeing als bei Airbus
                sein und das Modell gibt mit voller Sicherheit den Satz: “Das Flugzeug A350 wird hergestellt von
                Boeing.” aus.</p>

            <h4>Fehler in der Attention</h4>
            <p>Wir wissen, dass in dem <a href="/llm/index.html#attention-detail">Attention-Mechanismus</a> berechnet
                wird, wie
                stark jedes Wort im Satz mit jedem
                anderen Wort in Verbindung steht.</p>
            <p>Was nun passieren kann ist das sogenannte “snowballing”, bei dem Phänomen handelt es sich darum, dass
                der Attention-Algorithmus, wenn im Input eine falsche Prämisse steht, seinen Fokus darauf ausrichtet
                und generiert basierend auf dieser falschen Annahme weiter, um den Kontext kohärent zu halten. Das
                Modell priorisiert Kohärenz (dass der Satz flüssig klingt) über Korrektheit. Es achtet auf Muster,
                nicht auf Wahrheitsgehalt.</p>

            <h4>Fehler im MLP</h4>
            <p>Während die Attention Beziehungen zwischen Wörtern herstellt, vermuten Forscher, dass das eigentliche
                "Wissen" in den <a href="/llm/index.html#mlp-detail">MLPs</a> gespeichert ist, die auf die Attention
                folgen.</p>
            <p>Das Problem hierbei ist, dass falls Falsche Trainingsdaten verwendet werden die Gewichte des
                Netzwerks nicht mehr stimmen.</p>
            <p>Das tönt zwar nach einem einfachen Problem, da man einfach die Daten nach ihrer Richtigkeit
                überprüfen kann, bevor man diese zum Trainieren benutzt. Jedoch ist das Problem, das die Menge von
                Informationen, die gebracht werden, so gross ist, dass es unmöglich wäre für einen Menschen diese zu
                überprüfen. Zudem gibt es neue Modelle, die während der Ausführung Zugriff auf das Internet haben
                und somit kann man die Daten nicht überprüfen.</p>

            <div class="summary-section">
                <h4>Zusammenfassung der Fehlerquellen</h4>
                <ul>
                    <li><strong>Embedding:</strong> Sorgt für unscharfe Begriffsabgrenzungen (Airbus ≈ Boeing).</li>
                    <li><strong>Attention:</strong> Priorisiert flüssigen Text und Kohärenz über Fakten
                        ("Snowballing").</li>
                    <li><strong>MLP:</strong> Speichert Fakten nur als Wahrscheinlichkeiten, basierend auf den
                        Trainingsdaten.</li>
                </ul>
                <p>Deshalb hilft der Befehl “Mache keine Fehler” nicht – das Modell weiss nicht, dass es einen
                    Fehler macht, es wählt nur den Pfad der höchsten Wahrscheinlichkeit.</p>
            </div>
        </section>

        <!-- Current Solutions -->
        <section id="loesungen" class="tips">
            <h2 class="gradient-text" style="margin-top:40px;">Aktuelle Lösungsansätze</h2>
            <p>In diesem Abschnitt beschreiben wir aktuelle Ansätze, welche verfolgt werden, um Halluzinationen
                innerhalb von LLM’s zu minimieren.</p>

            <div id="rag">
                <h3>Retrieval Augmented Generation (RAG)</h3>
                <p>Retrieval Augmented Generation ermöglicht es zwischen dem User Prompt und dem eigentlichen Berechnen,
                    weiteren Kontext zum Prompt hinzuzufügen. Wenn nun ein Benutzer fragt “Wie viele Sitzplätze hat der
                    A320?” kann durch RAG automatisch der Kontext “Ein A320 hat 150-187 Sitzplätze” hinzufügen werden.
                    Dadurch erhält das LLM gerade sämtliche Fakten und muss nicht auf interne, beim Training gelernte,
                    Informationen (welche auch veraltet sein könnten) zurückgreifen.</p>

                <div class="nn-architecture-diagram">
                    <div class="rag-diagram">
                        <!-- User -->
                        <div class="rag-user">User</div>

                        <!-- Arrow -->
                        <div class="rag-arrow">
                            <span>Frage</span>
                            <div class="rag-arrow-line"></div>
                        </div>

                        <!-- System -->
                        <div class="rag-system-container">
                            <span class="rag-system-label">RAG System</span>
                            <!-- DB inside -->
                            <div class="rag-db">Datenbank</div>
                        </div>

                        <!-- Arrow -->
                        <div class="rag-arrow">
                            <span>Kontext</span>
                            <div class="rag-arrow-line"></div>
                            <span>+ Frage</span>
                        </div>

                        <!-- LLM -->
                        <div class="rag-llm">LLM</div>

                        <!-- Arrow -->
                        <div class="rag-arrow">
                            <span>Antwort</span>
                            <div class="rag-arrow-line"></div>
                        </div>
                    </div>
                </div>

                <p>Während Halluzinationen meistens behoben werden können, hat dieser Lösungsansatz jedoch ein grosses
                    Risiko für "Prompt Injections".
                    Dabei versucht der Angreifer den System Prompt des LLMs so zu manipulieren, dass es seine
                    ursprünglichen Instruktionen ignoriert und Dinge tut, die es eigentlich nicht tun sollte.
                    Da die Daten aus der Datenbank direkt (ohne Filterung) in den Prompt des LLMs geschrieben werden,
                    kann
                    ein Angreifer durch das Hinzufügen von bösartigen Daten in die Datenbank das LLM steuern.
                    Ein Beispiel für einen Angriffsvektor wäre eine bösartige E-Mail, die in einem Ticket-System
                    gespeichert wird und dann vom Support-Bot gelesen wird.</p>


                <h3 id="temperatur">Temperatur</h3>
                <p>Jedes LLM hat einen Temperatur-Parameter, welcher steuert, wie "kreativ" das Modell ist.
                    Eine tiefe Temperatur (nahe 0) macht das Modell deterministisch und faktisch.
                    Eine hohe Temperatur (nahe 1 oder höher) macht das Modell kreativer, erhöht aber das Risiko für
                    Halluzinationen massiv.
                    Für klassische RAG Systeme, welche Fakten wiedergeben sollen, wird meist eine Temperatur von 0
                    gewählt, um das Risiko so gering wie möglich zu
                    gestalten.</p>

                <!-- Visualization: Temperature -->
                <div class="nn-architecture-diagram">
                    <div class="temp-scale-container">
                        <div class="temp-header-row">
                            <span class="temp-text-left">Low Temp (0)</span>
                            <span class="temp-text-right">High Temp (1)</span>
                        </div>
                        <div class="temp-bar"></div>
                        <div class="temp-desc-row">
                            <span class="temp-text-left">Faktisch, Strikt<br>Deterministisch</span>
                            <span class="temp-text-right">Kreativ, Variabel<br>Halluzinationsrisiko</span>
                        </div>
                    </div>
                </div>
            </div>



            <div id="rlhf">
                <h3>Reinforcement Learning from Human Feedback (RLHF)</h3>
                <p>Ein grosser Vorteil von RLHF ist, dass Menschen direkt im Prozess involviert sind. Da sämtliche
                    Antworten von Menschen angeschaut und auch auf faktische Korrektheit überprüft werden, können
                    Halluzinationen direkt erkennt werden und schlecht bewertet werden. Dadurch werden diese weniger
                    oder sogar komplett aus dem System genommen. Bei grossen KI-Firmen wird dies schon längstens
                    umgesetzt, könnte hier durch noch längeres oder besseres RLHF mehr Fehlinformationen aus dem System
                    gefiltert werden.</p>
                <p>Es gibt Vermutungen, dass durch RLHF jedoch eine neue Form von Fehlinformationen im System auftauchen
                    können. Während dem RLHF wird das LLM trainiert, hilfreich zu sein und dem Menschen zuzustimmen.
                    Besonders bei Fragen, auf welche eine Antwort im RLHF gewählt wird, die faktisch nicht korrekt ist.
                    Beispielsweise lernt das LLM, dass es auf die Frage, wie ein Auto gestohlen werden kann, antwortet,
                    dass diese Handlung illegal ist. Rein faktisch ist diese Antwort jedoch inkorrekt, weil nicht
                    beschrieben ist, wie ein Auto gestohlen werden kann. Das führt zu einem Konflikt zwischen Wahrheit
                    und der Erwartung von uns Menschen, was dazu führen kann, dass das LLM die Antwort gibt, welche wir
                    erwarten und nicht die Wahrheit.</p>
            </div>



            <!-- Prompt Engineering -->
            <!-- Prompt Engineering -->
            <div id="prompt-engineering">
                <h3>Prompt Engineering</h3>
                <p>Ein weiterer schon stark verbreiteter Lösungsansatz, um Halluzinationen innerhalb einer LLM zu
                    vermeiden, ist das Prompt Engineering. Unter Prompt Engineering wird verstanden, Prompts so
                    anzupassen, dass das LLM die bestmögliche Antwort generiert. Für sämtliche Abfragen muss ein
                    Benutzer ein Prompt schreiben, somit gibt es hier grosses Optimierungspotential. Doch auch für die
                    LLM-Hersteller ist Prompt Engineering wichtig. Die Hersteller definieren sogenannte System-Prompts,
                    diese sind für den Endbenutzer unsichtbar und können nicht überschrieben werden, enthalten jedoch
                    wichtige Regeln oder Anweisung für die LLM. Ein typisches Beispiel für einen System-Prompt ist, dass
                    der Hersteller der LLM sagt, wie die LLM heisst oder worauf sie bei der Antwort-Generierung Wert
                    legen soll.</p>

                <h4>Tipps für den idealen Prompt-Aufbau</h4>
                <ul>
                    <li><strong>Abtrennungen:</strong> Verwende XML-Tags wie <code>&lt;text&gt;...&lt;/text&gt;</code>,
                        um Anweisungen klar von Daten zu trennen.</li>
                    <li><strong>Spezifischer Kontext:</strong> Gib relevante Daten, erwartete Ausgabeformate (z.B.
                        "Liste mit 5 Punkten") und Rollen ("Du bist ein Physiklehrer") an.</li>
                    <li><strong>Schritt für Schritt (Chain of Thought):</strong> Fordere das Modell auf, "Schritt für
                        Schritt" zu denken. Dies zwingt das Modell, Zwischenschritte zu berechnen, was logische Fehler
                        reduziert.</li>
                    <li><strong>Nach Beweisen fragen:</strong> Erlaube dem Modell explizit zu sagen "Ich weiss es
                        nicht", anstatt etwas zu erfinden.</li>
                </ul>
            </div>

            <!-- Agent Architecture -->
            <div id="agenten">
                <h3>Agent Architektur</h3>
                <p>Anstelle von einer einzigen LLM, welche den User Prompt verarbeitet, gibt es den Ansatz von KI-Agents
                    zu arbeiten. Dies sind Systeme, welche aus einer LLM bestehen, welche verschiedene Aufgaben
                    koordinieren kann. Zusätzlich sind oft andere Tools, wie Mail, Kalender oder Datenbanken daran
                    angeschlossen, um eine Antwort noch besser und fehlerfreier zu generieren.</p>

                <h4>Unternehmen Nachstellung (Multi-Agent System)</h4>
                <p>In dieser Architektur wird pro Abteilung in einem realen Unternehmen ein Agent erstellt, welcher in
                    seinem Bereich spezialisiert ist. Wenn nun ein Benutzer sein Prompt absetzt, wird dies dem
                    Projektleiter gegeben, welcher dann einen ersten Plan entwirft. Dieser wird dann den einzelnen
                    Agents gegeben und von dort an wie in einem echten Unternehmen abgearbeitet. Die verschiedenen
                    Agents können hierbei komplett frei miteinander kommunizieren.</p>

                <div class="nn-architecture-diagram">
                    <div class="agent-diagram">
                        <div class="agent-boss">Projektleiter</div>
                        <div class="agent-connectors">
                            <!-- Helper lines to mimic tree -->
                            <div class="agent-line-vertical"></div>
                            <div class="agent-line-horizontal"></div>
                            <div class="agent-line-drop-left"></div>
                            <div class="agent-line-drop-right"></div>
                        </div>

                        <div class="agent-row">
                            <div class="agent-node">
                                <span class="agent-title">Design Agent</span>
                                <span class="agent-sub">"Spezialist Grafik"</span>
                            </div>
                            <div class="agent-node">
                                <span class="agent-title">Handwerk Agent</span>
                                <span class="agent-sub">"Spezialist Möbel"</span>
                            </div>
                            <div class="agent-node">
                                <span class="agent-title">Finanz Agent</span>
                                <span class="agent-sub">"Spezialist Geld"</span>
                            </div>
                        </div>
                    </div>
                </div>

                <p>Ein grosser Vorteil von diesem Aufbau ist, dass jeder Agent auf ein Thema spezialisiert ist und nicht
                    alles verstehen muss. Die Aufgabe vom User wird in kleine Subtasks aufgeteilt, welche dann gezielt
                    und genauer abgearbeitet werden können.</p>
            </div>
        </section>
    </main>

    <div id="footer-placeholder"></div>

    <script src="/js/main.js"></script>
    <script src="/js/footer.js"></script>
</body>

</html>