<!DOCTYPE html>
<html lang="de">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDPA FLMI - Anfälligkeit</title>
    <link rel="stylesheet" href="/style.css">
</head>

<body>
    <div id="header-placeholder"></div>

    <main>
        <section class="hero">
            <h1 class="gradient-text">Anfälligkeit</h1>
            <p>Warum LLMs Fehler machen und was wir dagegen tun können.</p>
            <a href="#einleitung" class="button">Mehr erfahren</a>
        </section>

        <!-- Intro Section -->
        <section id="einleitung">
            <h2 class="gradient-text">Einleitung</h2>
            <h3>Halluzinationen</h3>
            <p>Large Language Models (LLM) wie ChatGPT sind noch nicht perfekt. Ein wichtiger Kritikpunkt ist, dass
                diese Modelle oft falsche Informationen erzeugen. Diese wirken auf den ersten Blick glaubwürdig. In der
                Fachliteratur nennt man dies Halluzination. Eine Studie der BBC hat gezeigt, dass es dieses Phänomen
                öfter gibt, als man denkt. In fast 45 % der Antworten von KI-Assistenten auf Nachrichtenfragen gibt es
                mindestens eine grosse Unstimmigkeit. Eine Untersuchung mit der European Broadcasting Union (EBU) zeigt:
                Die Technik ist nicht zuverlässig genug.</p>
            <p>Die sogenannte Autoritäts-Falle ist ein Problem: Die Modelle sind darauf trainiert, Informationen
                sachlich, selbstbewusst und seriös zu präsentieren. Der Leser kann nicht erkennen, ob die Antwort stimmt
                oder eine falsche Halluzination ist. In 31 % der Fälle haben die Forscher festgestellt, dass die von der
                KI angezeigten Quellen und Zitate falsch oder nicht relevant waren.</p>
            <p>Die Fehlerquote ist bei den verschiedenen Anbietern unterschiedlich hoch. ChatGPT und Microsoft Copilot
                hatten bei etwa einem Drittel der Antworten Mängel. Google Gemini hatte bei den Quellenangaben eine
                Fehlerquote von 76 %. Das zeigt, dass es immer noch sehr schwer ist, aktuelle Daten aus dem Internet in
                die Technik einzubauen. Die Studie zeigt auch, dass es Unterschiede in der Sprache gibt.
                Englischsprachige Anfragen sind deshalb sehr zuverlässig, kleinere europäische Sprachen sind weniger
                zuverlässig.</p>
            <p>Der Bericht zeigt, dass die Menschen immer weniger Vertrauen in Fakten haben. Wenn man immer mehr
                Informationen von KI-Systemen bekommt, kann man leicht falsche Informationen verbreiten. Das ist
                schlecht
                für die Qualität von Nachrichten. Die Experten der EBU und der BBC denken, dass KI-Assistenten im Moment
                eher als kreative Werkzeuge und nicht als verlässliche Fakten-Sammlungen benutzt werden sollten. Jedoch
                gibt es schon jetzt ein paar Methoden, um diese Fehlerquote zu verbessern, diese schauen wir uns in
                diesem
                Kapitel an.</p>
        </section>

        <!-- Technical Causes -->
        <section id="ursachen">
            <h2 class="gradient-text">Ursachen</h2>
            <h3>Warum passieren Fehler?</h3>
            <p>Dass Maschinen Fehler machen, scheint auf den ersten Blick einfach lösbar, denn eine Maschine macht ja
                nur das, was man ihr sagt, also kann man einer LLM nicht einfach sagen, dass sie keine Fehler machen
                soll? Die Antwort ist komplex, denn im Gegensatz zu herkömmlicher Software basieren LLMs nicht auf
                logischen Regeln, sondern auf statistischen Wahrscheinlichkeiten. Sie “wissen” nichts, sie berechnen
                lediglich die plausibelste Fortsetzung eines Textes.</p>
            <p>Wie im Teil <a href="/llm/index.html#training-detail">Training</a> beschrieben ist das fundamentale
                Problem das
                Trainingsziel:
                Next-Token-Prediction. Das Modell wird darauf trainiert, den statistischen Fehler (Cross-Entropy
                Loss) bei der Vorhersage des nächsten Wortes zu minimieren, nicht den faktischen Fehler.</p>

            <h4>Fehler im Embedding</h4>
            <p>Wie im <a href="/llm/index.html#vektoren">Embedding Teil</a> beschrieben werden Wörter, die in ähnlichen
                Kontexten
                vorkommen, im Vektorraum
                sehr nahe beieinander dargestellt. So zum Beispiel sind die Vektoren, welche die Wörter Airbus und
                Boeing darstellen sehr ähnlich. Wenn ein Modell jetzt halluziniert, greift es nicht komplett
                daneben, sondern nur ein bisschen, also kann zum Beispiel beim Satz, “Das Flugzeug A350 wird
                hergestellt von” das nächste Wort in der vektoriellen Darstellung näher bei Boeing als bei Airbus
                sein und das Modell gibt mit voller Sicherheit den Satz, “Das Flugzeug A350 wird hergestellt von
                Boeing.” aus.</p>

            <h4>Fehler in der Attention</h4>
            <p>Wir wissen, dass in dem <a href="/llm/index.html#attention-detail">Attention-Mechanismus</a> berechnet
                wird, wie
                stark jedes Wort im Satz mit jedem
                anderen Wort in Verbindung steht.</p>
            <p>Was nun passieren kann, ist das sogenannte “snowballing”. Sobald beim Attention Input eine falsche
                Aussage steht, wird der Fokus der Attention danach ausgerichtet. Das LLM generiert dann sämtlichen
                folgenden Text aufgrund dieser falschen Annahme weiter. Das Modell priorisiert den flüssigen Satz
                gegenüber Korrektheit. Es achtet auf Muster, nicht auf den Wahrheitsgehalt, des Textes.</p>

            <h4>Fehler im MLP</h4>
            <p>Während die Attention Beziehungen zwischen Wörtern herstellt, vermuten Forscher, dass das eigentliche
                "Wissen" in den <a href="/llm/index.html#mlp-detail">MLPs</a> gespeichert ist, die auf die Attention
                folgen.</p>
            <p>Das Problem hierbei ist, dass falls falsche Trainingsdaten verwendet werden, die Gewichte des
                Netzwerks nicht mehr stimmen.</p>
            <p>Das tönt zwar nach einem einfachen Problem, da man einfach die Daten nach ihrer Richtigkeit
                überprüfen kann, bevor man diese zum Trainieren benutzt. Jedoch ist das Problem, dass die Menge von
                Informationen, die gebracht werden, so gross ist, dass es unmöglich wäre für einen Menschen diese zu
                überprüfen. Zudem gibt es neue Modelle, die während der Ausführung Zugriff auf das Internet haben
                und somit kann man die Daten nicht überprüfen.</p>

            <div class="summary-section">
                <h4>Zusammenfassung der Fehlerquellen</h4>
                <ul>
                    <li><strong>Embedding:</strong> Sorgt für unscharfe Begriffsabgrenzungen (Airbus ≈ Boeing).</li>
                    <li><strong>Attention:</strong> Priorisiert flüssigen Text und Kohärenz über Fakten
                        ("Snowballing").</li>
                    <li><strong>MLP:</strong> Speichert Fakten nur als Wahrscheinlichkeiten, basierend auf den
                        Trainingsdaten.</li>
                </ul>
                <p>Deshalb hilft der Befehl “Mache keine Fehler” nicht – das Modell weiss nicht, dass es einen
                    Fehler macht, es wählt nur den Pfad der höchsten Wahrscheinlichkeit.</p>
            </div>
        </section>

        <!-- Current Solutions -->

    </main>

    <div id="footer-placeholder"></div>

    <script src="/js/main.js"></script>
    <script src="/js/footer.js"></script>
</body>

</html>