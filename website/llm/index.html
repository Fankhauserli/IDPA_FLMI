<!DOCTYPE html>
<html lang="de">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDPA FLMI - LLM</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/responsive.css">
</head>

<body>
    <div id="header-placeholder"></div>

    <main>
        <section class="hero">
            <h1>LLM</h1>
            <section class="llm-info-box">
                <p>In diesem Abschnitt wird beschrieben, wie ein LLM funktioniert.</p>
            </section>

            <section class="neural-network-section">
                <div class="tip-box">
                    <h2>Neurale Netzwerke</h2>
                    <p>Ein künstliches neuronales Netzwerk (KNN) ist der Versuch, die Funktionsweise des menschlichen
                        Gehirns mathematisch nachzubauen, um Computer dazu zu bringen, komplexe Muster zu erkennen.
                        Während klassische Computerprogramme stur einer programmierten Logik folgen (Wenn A, dann B),
                        lernt ein neuronales Netzwerk aus Erfahrungen, ähnlich wie ein Kind lernt, eine Katze von einem
                        Hund zu unterscheiden.</p>

                    <div class="nn-architecture-diagram">
                        <svg viewBox="0 0 600 300" preserveAspectRatio="xMidYMid meet">
                            <!-- Connections first so they are behind nodes -->

                            <!-- Input to Hidden 1 -->
                            <line x1="80" y1="80" x2="250" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="80" x2="250" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="80" x2="250" y2="240" stroke="#ccc" stroke-width="1" />

                            <line x1="80" y1="150" x2="250" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="150" x2="250" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="150" x2="250" y2="240" stroke="#ccc" stroke-width="1" />

                            <line x1="80" y1="220" x2="250" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="220" x2="250" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="80" y1="220" x2="250" y2="240" stroke="#ccc" stroke-width="1" />

                            <!-- Hidden 1 to Hidden 2 -->
                            <line x1="250" y1="60" x2="400" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="60" x2="400" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="60" x2="400" y2="240" stroke="#ccc" stroke-width="1" />

                            <line x1="250" y1="150" x2="400" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="150" x2="400" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="150" x2="400" y2="240" stroke="#ccc" stroke-width="1" />

                            <line x1="250" y1="240" x2="400" y2="60" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="240" x2="400" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="250" y1="240" x2="400" y2="240" stroke="#ccc" stroke-width="1" />

                            <!-- Hidden 2 to Output -->
                            <line x1="400" y1="60" x2="520" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="400" y1="150" x2="520" y2="150" stroke="#ccc" stroke-width="1" />
                            <line x1="400" y1="240" x2="520" y2="150" stroke="#ccc" stroke-width="1" />

                            <!-- Input Layer -->
                            <circle cx="80" cy="80" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="80" cy="150" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="80" cy="220" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <text x="80" y="270" text-anchor="middle" font-weight="bold">Input Layer</text>

                            <!-- Hidden Layer 1 -->
                            <circle cx="250" cy="60" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="250" cy="150" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="250" cy="240" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />

                            <!-- Hidden Layer 2 -->
                            <circle cx="400" cy="60" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="400" cy="150" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <circle cx="400" cy="240" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <text x="325" y="280" text-anchor="middle" font-weight="bold">Hidden Layers</text>

                            <!-- Output Layer -->
                            <circle cx="520" cy="150" r="20" fill="white" stroke="#4d25fc" stroke-width="2" />
                            <text x="520" y="200" text-anchor="middle" font-weight="bold">Output Layer</text>
                        </svg>
                    </div>

                    <div>
                        <h3>Gewichte und Parameter</h3>
                        <p>Das wichtigste Konzept zum Verständnis sind die Verbindungen zwischen den Neuronen. Jedes
                            Neuron ist mit den Neuronen der nächsten Schicht verbunden. Diese Verbindungen sind jedoch
                            nicht alle gleich stark. Jede Verbindung hat ein sogenanntes Gewicht (Weight).</p>
                        <p>Man kann sich diese Gewichte wie Wasserleitungen mit Ventilen vorstellen: Bei manchen
                            Verbindungen ist das Ventil weit offen (das Signal kommt stark durch), bei anderen ist es
                            fast zu (das Signal wird blockiert).</p>
                    </div>

                    <!-- Interactive Valve Element -->
                    <div class="valve-interactive-container">
                        <h4>Interaktives Element: Das Ventil</h4>
                        <div class="valve-visual">
                            <div class="valve-node">
                                Input
                                <span class="valve-value-label" id="valve-input-value">1.0</span>
                            </div>
                            <div class="valve-connection-wrapper">
                                <span class="valve-value-label valve-weight-label" id="valve-weight-value">Weight:
                                    1.0</span>
                                <div id="valve-connection"></div>
                            </div>
                            <div class="valve-node">
                                Output
                                <span class="valve-value-label" id="valve-output-value">1.0</span>
                            </div>
                        </div>
                        <div class="valve-controls">
                            <input type="range" id="valve-slider" class="valve-slider" min="-5" max="5" step="0.1"
                                value="1">
                            <p>Stell dir das Gewicht als Ventil vor. Verändere das Gewicht, um zu sehen, wie viel vom
                                Input beim Output ankommt.</p>
                        </div>
                    </div>

                    <p>Zusammengenommen bilden diese Gewichte und Einstellungen die Parameter des Modells. Wenn wir
                        später davon sprechen, dass ein LLM «Milliarden von Parametern» hat, meinen wir genau diese
                        Milliarden von kleinen Stellschrauben (unseren Ventilen) und Verbindungen innerhalb des
                        neuronalen Netzwerks, die den Informationsfluss steuern.</p>

                    <p>Das Ziel des Netzwerks ist es, diese Parameter so einzustellen, dass am Ende das richtige
                        Ergebnis herauskommt. Ein LLM ist im Grunde nichts anderes als ein gigantisches, hochkomplexes
                        neuronales Netzwerk mit einer speziellen Architektur, die sich besonders gut für Sprache eignet.
                    </p>

                    <div>
                        <h3>Die Mathematik hinter einem Neuron</h3>
                        <p>Um zu verstehen, wie das Netzwerk "denkt", müssen wir uns ansehen, was in einem einzelnen
                            Neuron mathematisch passiert. Es ist weniger kompliziert, als es aussieht. Der Prozess in
                            einem einzelnen Neuron lässt sich in zwei Schritte unterteilen: die lineare Funktion und die
                            Aktivierungsfunktion.</p>

                        <h4>1. Die gewichtete Summe (Linearer Teil)</h4>
                        <p>Stellen wir uns ein Neuron vor, das drei Eingaben erhält (z. B. drei Wörter oder Pixel). Jede
                            Eingabe (x) wird mit ihrem eigenen Gewicht (w) multipliziert. Das ist genau der Effekt des
                            oben beschriebenen "Ventils". Anschliessend werden alle Ergebnisse zusammengezählt.</p>
                        <p>Mathematisch sieht das für ein einzelnes Neuron so aus:</p>
                        <p
                            style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                            z = (x1 · w1) + (x2 · w2) + (x3 · w3) + b</p>
                        <p>Oder vereinfacht mit dem Summenzeichen ∑:</p>
                        <p
                            style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                            z = ∑(xi · wi) + b</p>
                        <ul>
                            <li><strong>x (Input):</strong> Die Information, die reinkommt.</li>
                            <li><strong>w (Weight):</strong> Die Wichtigkeit der Information (unser Ventil).</li>
                            <li><strong>b (Bias):</strong> Der Schwellenwert.</li>
                        </ul>
                        <p><strong>Der Bias (b):</strong> Der Bias ist ein spezieller Parameter, der oft vergessen wird.
                            Er ist eine Art "Grundstimmung" des Neurons. Selbst wenn alle Eingaben (x) null sind, kann
                            der Bias dafür sorgen, dass das Neuron trotzdem feuert. Er verschiebt die Aktivierungskurve
                            nach oben oder unten, ähnlich wie der y-Achsenabschnitt in einer einfachen Geradengleichung
                            (y = mx + q).</p>

                        <h4>2. Die Aktivierungsfunktion (Nicht-linearer Teil)</h4>
                        <p>Nachdem die gewichtete Summe (z) berechnet wurde, ist das Ergebnis oft eine beliebige Zahl
                            zwischen minus unendlich und plus unendlich. Damit das Netzwerk komplexe Probleme lösen
                            kann, muss entschieden werden: "Ist dieses Signal stark genug, um weitergeleitet zu
                            werden?".</p>
                        <p>Hier kommt die Aktivierungsfunktion (f) ins Spiel. Sie nimmt das Ergebnis der Summe und
                            wandelt es um:</p>
                        <p
                            style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                            output = f(z)</p>
                        <p>In modernen LLMs wird hier oft die ReLU-Funktion (Rectified Linear Unit) verwendet. Sie ist
                            mathematisch sehr simpel: Sie wandelt alle negativen Zahlen in Null um.</p>
                        <p
                            style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                            f(z) = max(0, z)</p>
                        <p>Das bedeutet: Wenn die gewichtete Summe negativ ist, bleibt das Neuron "still" (Output 0).
                            Ist sie positiv, leitet es das Signal weiter. Ohne diese nicht-linearen Funktionen wäre das
                            gesamte Netzwerk, egal wie gross, nur eine einfache lineare Regression und könnte keine
                            Sprache verstehen.</p>
                    </div>
                </div>
            </section>

        </section>

        <section class="tips">
            <div class="tip-box">
                <h3>LLM-Aufbau</h3>
                <p>Ein LLM kann sehr unterschiedlich aufgebaut sein. Die bekanntesten LLM-Architekturen heissen
                    Transformer, Mamba und RNN (Rekurrente Neurale Netze). RRN wurden durch Transformer abgelöst, dabei
                    wurden zwei grosse Probleme behoben. Transformer laufen im Vergleich zu RRN parallel, somit können
                    sie einen Text gelichzeitig abarbeiten und gehen nicht Wort für Wort vor. Zusätzlich können sich
                    Transformer an sämtliche Informationen im Text erinnern, während RRN die Informationen, wenn der
                    Text zu lange war, nach und nach wieder vergassen. Die Transformer Architektur ist aktuell die
                    meistverbreitete Architektur und wird in ChatGPT, Copilot, Gemini und vielen weiteren Chatbots
                    verwendet. Mamba ist eine Architektur, die gerade erst entwickelt wird. Hier wird versucht den
                    Rechenaufwand, der bei Transformer quadratisch zur Textlänge steigt, linear zur Textlänge gestalten.
                    Innerhalb dieser IDPA werden wir nur auf die Transformer Architektur weiter eingehen, da diese
                    aktuell am meisten verwendet wird.</p>
                <p>Bei allen folgenden Informationen zum Transformer Aufbau muss beachtet werden, dass diese stark
                    vereinfacht sind. Da LLM ihre Modell-Parameter selbst einstellen, kann zwar erklärt werden, welche
                    Mathematische Funktion ausgeführt werden, was diese aber schlussendlich wirklich bewirken, bleibt
                    selbst für Expert:innen oft eine Vermutung.</p>

                <p>Grundsätzlich ist ein Transformer in vier Teilschritte trennbar. Im ersten Teil wandelt der
                    Transformer die, für uns Menschen verständliche, Wörter aus der Eingabe in Computer verständliche
                    Worte um. Dieser Schritt wird Embedding genannt. Anschliessend in der sogenannten Attention
                    berechnet der Transformer den Kontext der einzelnen Worte. Dieser Schritt ermöglicht es, komplexere
                    Wörter oder Sätze zu verstehen. So gibt es zum Beispiel Wörter, welche mehrere Bedeutungen haben,
                    wobei die richtige Bedeutung erst durch den Kontext hervorkommt. Ein Beispiel für ein solches Wort
                    ist die “Bank”. Im nächsten Schritt hat die LLM dann noch die Chance das Wort, welches generiert
                    wird, mit eigenen Informationen anzureichern. Bei diesem Schritt gibt es verschiedene
                    Lösungsansätze, wir schauen uns nur das MLP (Multilayer Perzeptron) genauer an. Nach diesem Schritt
                    ist die generierung des Wortes abgeschlossen und das Wort wird wieder umgewandelt in ein, für uns
                    Menschen, verständliches Wort.</p>
            </div>

            <div class="tip-box">
                <h3>Bedeutung in Vektoren</h3>
                <p>Ein Wort erhält im Sprachmodell seine Bedeutung nicht in sich selbst, sondern aus den vielen
                    Beziehungen, die es zu anderen Wörtern hat. Die Vektoreneinbettung ist das Zuteilen der Bedeutung zu
                    Wörtern, indem es gegen andere ähnliche Wörter verglichen wird. Sie erhalten einen eigenen Wert, der
                    dann durch den Kontext anderer Wörter in einem Satz ergänzt wird.</p>
                <p>Wie kann nun die Bedeutung eines Wortes in Werten festgehalten werden? In LLMs wird dies mit
                    hochdimensionalen Vektoren gemacht.</p>
                <p>Ein Beispiel für die Festhaltung von nicht numerischer Bedeutung in Vektoren ist wie Computer mit
                    Farben im RGB-Profil umgehen. Diese werden in einem 3-dimensionalen Vektor festgehalten.</p>
                <p>Wir haben 3 Dimensionen, drei «Richtungen». Rot, Grün und Blau. Die Werte gehen von 0-255. Der
                    Computer versteht nicht, dass dies die Farbe Rot ist, doch er weiss, wo Rot im 3-dimensionalen Raum
                    liegt. (255, 0, 0) bedeutet “Die Roten Pixel sollen auf der höchsten Helligkeit sein”.</p>

                <div class="tip-box-special">
                    <h4>Interaktives Element: Farben als Vektoren (3D)</h4>
                    <!-- 3D Canvas Container -->
                    <div id="rgb-cube-container"
                        style="width: 100%; height: 400px; background-color: #E5E7EB; border-radius: 10px; margin-bottom: 20px; position: relative;">
                        <p style="text-align:center; padding-top: 180px; color: #999;">Lade 3D Ansicht...</p>
                    </div>

                    <div class="rgb-controls">
                        <div class="rgb-slider-row">
                            <span class="rgb-label" style="color: #ff0000;">Rot (X)</span>
                            <input type="range" class="rgb-slider-input" id="slider-r" min="0" max="255" value="0">
                            <span class="rgb-value" id="val-r">0</span>
                        </div>
                        <div class="rgb-slider-row">
                            <span class="rgb-label" style="color: #00ff00;">Grün (Y)</span>
                            <input type="range" class="rgb-slider-input" id="slider-g" min="0" max="255" value="0">
                            <span class="rgb-value" id="val-g">0</span>
                        </div>
                        <div class="rgb-slider-row">
                            <span class="rgb-label" style="color: #0000ff;">Blau (Z)</span>
                            <input type="range" class="rgb-slider-input" id="slider-b" min="0" max="255" value="0">
                            <span class="rgb-value" id="val-b">0</span>
                        </div>
                    </div>
                    <div class="rgb-vector-display">
                        Vektor: (<span id="vec-r">0</span>, <span id="vec-g">0</span>, <span id="vec-b">0</span>)
                    </div>
                    <p style="margin-top: 20px; font-size: 0.9em; font-style: italic;">Der Vektor zeigt vom Ursprung
                        (Schwarz) zur gemischten Farbe im RGB-Würfel. Rotieren & Zoomen möglich.</p>
                </div>

                <!-- Import Map for Three.js -->
                <script type="importmap">
                    {
                        "imports": {
                            "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                            "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
                        }
                    }
                </script>
                <!-- 3D Logic Script -->
                <script type="module" src="/js/rgb_cube.js?v=2"
                    onerror="document.getElementById('rgb-3d-container').innerHTML = '<p style=\'padding:20px; color:red\'>Fehler beim Laden von 3D. Bitte Internetverbindung prüfen.</p>'"></script>

                <p>Ein Computer versteht so welche Farbe es anzeigen soll, somit “Weiss” der Computer, was Blau ist, und
                    kann es anzeigen. Er versteht sogar den Unterschied zwischen Hellblau und Dunkelblau, da der Vektor
                    der Farbe einfach weiter Richtung weiss zeigt. Er weiss nicht, was Rot wirklich ist, doch kennt
                    seinen Vektor. So wird eine Bedeutung in einem Vektor festgehalten.</p>
                <p>Wörter sind für LLMs genauso Vektoren, nur dass diese durch die erhöhte Komplexität der Bedeutung von
                    Wörtern deutlich mehr Dimensionen haben, welche dem LLM Helfen “Attribute” der Wörter festzuhalten.
                    Bei modernen LLMs sind dies ungefähr 2’000-10’000 Dimensionen. Diese Dimensionen sind nicht benannt
                    wie in unserem Beispiel der Farben (Richtung Rot, Grün und Blau), doch das LLM weiss was diese
                    bedeuten.</p>
                <p>Die Wörter müssen zuerst ihre Werte erhalten. Dies geschieht in der Vektoreneinbettung. Durch
                    “Training”, wo einem LLM viele Textdaten gegeben werden, schaut das LLM jedes einzelne Wort in
                    seinem Kontext an und generiert dafür einen Vektor der die Bedeutung durch die Abhängigkeit und
                    Ähnlichkeit zu anderen Wörtern erlangt.</p>
            </div>

            <div class="tip-box">
                <h3>Attention</h3>
                <p>Im Einbettungsschritt haben wir den Input ins LLM in verschiedenen Token aufgetrennt und für jeden
                    Token ein Vektor erhalten. Dies ist jedoch noch sehr linear, das Wort Flugzeug wird immer als
                    gleicher Vektor interpretiert. Die unterschiedlichen Hersteller oder Typen können nicht
                    widergespiegelt werden. Damit die Bedeutung des Wortes genauer definiert werden kann, muss der
                    Kontext vom Text angeschaut werden. In der Attention können die anderen Tokens, welche auch
                    eingebettet wurden, den Vektor für “Flugzeug” beeinflussen. Somit kann Kontext vom Rest des Textes
                    zu einem Wort hinzugefügt werden. In diesem Abschnitt werden wir nur die Self-Attention genauer
                    anschauen, die Cross-Attention lassen wir aus.</p>

                <h4>Query und Key Vektoren</h4>
                <p>Innerhalb der Attention wird mit jedem Token zwei Matrixmultiplikationen durchgeführt. Beide Matrizen
                    bestehen komplett aus Modelparameter, welche, während dem Lernen der LLM definiert werden. Als
                    Ergebnis der beiden Multiplikationen erhalten wir Query und Key Vektoren, diese befinden sich in
                    einer kleineren Dimension im Vergleich zum ursprünglichen Einbettungsraum.</p>
                <p>Die erste Matrixmultiplikation ergibt einen Query-Vektor, welcher eine Frage an die umliegenden
                    Tokens widerspiegelt. Ein sehr stark vereinfachtes Beispiel ist, dass die Matrik, welche bei der
                    Matrixmultiplikation für den Query-Vektor verwendet wird, bei Nomen den Fragenvektor “Gibt es ein
                    Flugzeug Typ vor mir?” zurückgibt.</p>
                <p>Bei der zweiten Matrixmultiplikation erhalten wir einen Key-Vektor. Der Key-Vektor ist die Antwort
                    zur Frage des Query-Vektors. Hier würden in unserem Beispiel Tokens, welche einen Flugzeug Typ
                    repräsentieren, die Antwort geben “Ich bin ein Flugzeug Typ, an der Position X”.</p>

                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 600 200">
                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4d25fc" />
                            </marker>
                        </defs>
                        <!-- Token -->
                        <rect x="50" y="80" width="80" height="40" rx="5" fill="#e6e0f8" stroke="#4d25fc"
                            stroke-width="2" />
                        <text x="90" y="105" text-anchor="middle" font-family="sans-serif" font-weight="bold"
                            fill="#4d25fc">Token</text>

                        <!-- Weights Q -->
                        <rect x="200" y="40" width="60" height="40" fill="white" stroke="#333" stroke-width="1" />
                        <text x="230" y="65" text-anchor="middle" font-family="sans-serif" font-size="12">Weight
                            Q</text>
                        <!-- Weights K -->
                        <rect x="200" y="120" width="60" height="40" fill="white" stroke="#333" stroke-width="1" />
                        <text x="230" y="145" text-anchor="middle" font-family="sans-serif" font-size="12">Weight
                            K</text>

                        <!-- Arrows -->
                        <line x1="130" y1="100" x2="200" y2="60" stroke="#4d25fc" stroke-width="2"
                            marker-end="url(#arrowhead)" />
                        <line x1="130" y1="100" x2="200" y2="140" stroke="#4d25fc" stroke-width="2"
                            marker-end="url(#arrowhead)" />

                        <!-- Vectors Q & K -->
                        <rect x="350" y="40" width="100" height="30" rx="5" fill="#a777d1" stroke="none" />
                        <text x="400" y="60" text-anchor="middle" fill="white" font-weight="bold">Query (Q)</text>

                        <rect x="350" y="120" width="100" height="30" rx="5" fill="#a777d1" stroke="none" />
                        <text x="400" y="140" text-anchor="middle" fill="white" font-weight="bold">Key (K)</text>

                        <!-- Matrix Multi labels -->
                        <text x="280" y="55" font-family="sans-serif" font-size="20">×</text>
                        <text x="280" y="135" font-family="sans-serif" font-size="20">×</text>
                    </svg>
                </div>

                <h4>Skalarprodukt vom Key und Query Vektor</h4>
                <p>Sobald für jeden Token ein Key und Query-Vektor berechnet wurde, wird das Skalarprodukt von jedem
                    Key-Vektor mit jedem Query-Vektor ausgerechnet. Wenn das Skalarprodukt von einem Query-Vektor und
                    einem Key-Vektor stark positiv ist, bedeutet dies, dass das Token vom Key-Vektor auf das Token vom
                    Query-Vektor Einfluss haben sollte. Bei einem Skalarprodukt nahe bei Null oder im Minus beeinflussen
                    sich die Token gegenseitig nicht oder nur sehr schwach. Bei unserem Beispiel würde das Skalarprodukt
                    von einem Flugzeug Typ darstellendem Key-Vektor und einem “Flugzeug” Query-Vektor ein sehr hohes
                    Skalarprodukt ergeben und somit aufzeigen, dass ein Flugzeug Typ das Wort “Flugzeug” stark
                    beeinflusst.</p>

                <h4>Softmax</h4>
                <p>Sämtliche Skalarprodukte von Key-Vektoren, welche nach einem Query-Vektor in der Reihenfolge sind,
                    werden auf minus Unendlich gesetzt. Dies bewirkt, dass in folgendem Beispielsatz: “Das A320 Flugzeug
                    neben dem PC-21” das Token für “PC-21” das Token für “Flugzeug” nicht beeinflusst.</p>
                <p>Anschliessend wird ein Softmax von allen Skalaren von einem Query-Vektor und allen Key-Vektoren
                    genommen. Softmax setzt alle Werte im Minus Bereich auf Null und gewichtet alle positiven Zahlen so,
                    dass am Schluss die Summe sämtlicher Zahlen zusammenaddiert Eins ergeben. Das Ergebnis vom Softmax
                    ist eine normalisierte Spalte, in welcher abgelesen werden kann welcher Token welchen Token wie fest
                    beeinflussen muss.</p>

                <h4>Beeinflussung</h4>
                <p>Nun wissen wir, zum Beispiel, dass “A320” die Einbettung für “Flugzeug” beeinflussen muss. Für die
                    eigentlich Beeinflussung muss nun die Einbettung vom "Flugzeug” Token angepasst werden. Hier wird
                    eine weitere Matrixmultiplikation gerechnet. Auch bei dieser besteht die Matrix aus Modelparameter,
                    die wiederum, während dem Lernen gesetzt werden. Die Einbettung von jedem Token wird mit der Matrik
                    multipliziert, draus erhält man einen weiteren Vektor, der aufzeigt, was bei einem anderen Token
                    addiert werden muss, um diesen zu beeinflussen. Diesen Vektor nennen wir von nun an
                    “Beeinflussungs-Vektor”. Bei unserem Beispiel würde die Multiplikation, der Matrix, mit dem Vektor
                    für “A320” ein Beeinflussungs-Vektor ergeben, welcher zu “Flugzeug” addiert werden kann. Das
                    Ergebnis daraus wäre “Flugzeug” mit dem Kontext “A320”.</p>

                <p>Da wir mehrere Tokens haben, welche unterschiedlich stark beeinflussen, müssen sämtliche
                    Beeinflussungs-Vektoren mit dem Softmax Ergebnis des gleichen Key-Token multipliziert werden. Nun
                    können alle Ergebnisse für ein Query-Token zusammenaddiert werden, um einen Vektor zu erhalten,
                    welcher die unterschiedlich stark Gewichtete Beeinflussungen beinhaltet. In unserem Beispiel könnte
                    also mit einem Vektor zum Token “Flugzeug” den Kontext “weiss, A320, Airbus” dazu addiert werden,
                    wobei die einzelnen Kontextwörter unterschiedlich gewichtet wurden.</p>

                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 600 200">
                        <defs>
                            <marker id="arrowhead-b" markerWidth="10" markerHeight="7" refX="9" refY="3.5"
                                orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4d25fc" />
                            </marker>
                        </defs>
                        <!-- Original Token -->
                        <g transform="translate(50, 80)">
                            <rect width="80" height="40" rx="5" fill="#ddd" stroke="#999" stroke-width="2" />
                            <text x="40" y="25" text-anchor="middle" font-size="12">Original</text>
                        </g>

                        <!-- Context Vector -->
                        <g transform="translate(200, 80)">
                            <rect width="80" height="40" rx="5" fill="#e6e0f8" stroke="#4d25fc" stroke-width="2" />
                            <text x="40" y="25" text-anchor="middle" font-size="12">Kontext (A320)</text>
                        </g>

                        <!-- Added -->
                        <g transform="translate(450, 80)">
                            <rect width="100" height="40" rx="5" fill="#a777d1" stroke="none" />
                            <text x="50" y="25" text-anchor="middle" fill="white" font-weight="bold">Neuer Vektor</text>
                        </g>

                        <text x="170" y="105" font-size="24">+</text>
                        <line x1="290" y1="100" x2="440" y2="100" stroke="#4d25fc" stroke-width="2"
                            marker-end="url(#arrowhead-b)" />
                    </svg>
                </div>

                <p>Nun konnten wir erfolgreich Kontext, aufgrund anderen Tokens, zu einem zuvor kontextlosen Token
                    hinzufügen.</p>
            </div>

            <div class="tip-box">
                <h3>Multi Headed Attention</h3>
                <p>Sämtliche Schritte, welche nun erklärt wurden, werden in der Fachsprache einen “Head” genannt. In
                    fast allen LLM’s gibt es nicht nur einen “Head” in der Attention, sondern viele aneinander gereihte
                    “Head”. Dies wird dann Multi Headed Attention genannt. Der Hauptgrund, wieso eine Multi Headed
                    Attention durchgeführt wird, ist, dass bei jedem einzelnen “Head” neue Matrizen verwendet werden,
                    welche aus unterschiedlichen Modelparamter bestehen. Somit kann Kontext noch schneller und genauer
                    bei Tokens hinzugefügt werden.</p>
            </div>

            <div class="tip-box">
                <h3>Multilayer Perzeptron (MLP)</h3>
                <p>Das MLP ist ein Neurales Netzwerk, welches dem LLM ermöglicht Informationen zu speichern. Ein
                    Anschauliches Beispiel ist, wenn man ein ChatBot fragt, wie viel Personen in ein Airbus A320 passen.
                    Solange das LLM keinen Zugriff aufs Internet hat, muss die Sitzplatzanzahl innerhalb des Models
                    gespeichert sein. Genau diesen Schritt übernimmt das MLP. Jeder Vektor, aus der Attention,
                    durchläuft die MLP-Schicht parallel und isoliert von sämtlichen anderen Vektoren. Damit wir den
                    Aufbau des MLP besser verstehen ignorieren wir die Parallelität und folgen schrittweise einem
                    Durchlauf durchs MLP.</p>

                <h4>Input ins MLP (Matrix Multiplikation)</h4>
                <p>Nach der Attention haben wir mehrere Vektoren erhalten, welche von anderen Tokens beeinflusst wurden.
                    Da in der MLP-Schicht alle Vektoren parallel und isoliert voneinander verarbeitet werden, werden wir
                    zur Vereinfachung nur einen Vektor, welcher den Airbus A320 widerspiegelt, anschauen.</p>
                <p>Als erster Schritt innerhalb des MLPs wird der Vektor aus der Attention mit einer Matrix
                    multipliziert. Bei der Matrix handelt es sich um viele Modelparameter, die, während des Lernens der
                    LLM, definiert werden. Innerhalb der Matrix betrachten wir nun eine Zeile als einen weiteren Vektor.
                    Bei der Matrixmultiplikation erhalten wir somit das Skalarprodukt des Airbus A320 Vektors und einem
                    imaginieren Vektor, welcher aus einer Reihe innerhalb der Matrix besteht.</p>

                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 200" preserveAspectRatio="xMidYMid meet">
                        <!-- Vector -->
                        <rect x="50" y="50" width="20" height="100" fill="#e6e0f8" stroke="#4d25fc" />
                        <text x="60" y="40" text-anchor="middle" font-size="12">Vektor</text>
                        <!-- Matrix -->
                        <g transform="translate(120, 50)">
                            <rect width="100" height="100" fill="#fff" stroke="#333" />
                            <!-- Grid lines -->
                            <line x1="0" y1="20" x2="100" y2="20" stroke="#ddd" />
                            <line x1="0" y1="40" x2="100" y2="40" stroke="#ddd" />
                            <line x1="0" y1="60" x2="100" y2="60" stroke="#ddd" />
                            <line x1="0" y1="80" x2="100" y2="80" stroke="#ddd" />
                            <line x1="20" y1="0" x2="20" y2="100" stroke="#ddd" />
                            <line x1="40" y1="0" x2="40" y2="100" stroke="#ddd" />
                            <line x1="60" y1="0" x2="60" y2="100" stroke="#ddd" />
                            <line x1="80" y1="0" x2="80" y2="100" stroke="#ddd" />
                        </g>
                        <text x="170" y="40" text-anchor="middle" font-size="12">Parameter Matrix</text>
                        <text x="100" y="105" font-size="20">×</text>
                        <text x="250" y="105" font-size="20">=</text>
                        <!-- Result -->
                        <rect x="280" y="20" width="20" height="160" fill="#a777d1" stroke="none" />
                        <text x="290" y="195" text-anchor="middle" font-size="12">Grösserer Vektor</text>
                    </svg>
                </div>

                <p>Das Skalarprodukt von zwei Vektoren liefert Informationen darüber, wie Vektoren zueinanderstehen. Ist
                    das Ergebnis positiv, bedeutet dies, dass eine Ähnlichkeit zwischen zwei Vektoren besteht. Ein
                    Betrag im Minusbereich zeigt auf, dass die Vektoren gegensätzlich sind. Falls das Ergebnis genau
                    null beträgt, haben die Vektoren keine Gemeinsamkeiten. Wenn die beiden Vektoren genau
                    übereinstimmen, erhält man den Betrag, von einem der beiden Vektoren, im Quadrat. Sind die Vektoren
                    genaue Gegenteile erhält man die Quadratzahl im Minus.</p>
                <p>Um unser Beispiel möglichst verständlich zu halten, gehen wir davon aus, dass sowohl der Vektor aus
                    der Attention wie auch der Vektor aus der Matrix Einheitsvektoren sind.</p>
                <p>Jeder Vektor aus der Matrix kann als Frage angesehen werden. Diese Matrixmultiplikation ermöglicht
                    es, Fragen zu dem Vektor, welchen wir aus der Attention erhalten haben, zu stellen. Anhand des
                    Ergebnisses des Skalarproduktes erkennen wir, ob eine Übereinstimmung oder Gegensätzlichkeit
                    besteht.</p>

                <h4>ReLU (Aktivierung)</h4>
                <p>Der erste Schritt innerhalb des MLP ergab einen Vektor mit Zahlen. Jede Zahl sagt aus, wie fest eine
                    gestellte Frage mit dem ursprünglichen Vektor aus der Attention übereinstimmt. Um zu definieren,
                    wann eine Frage zutrifft, und wann diese nicht zutrifft, wird eine nicht lineare Funktion verwendet.
                    Diese Funktion ist unterschiedlich je nach LLM. Während unserer Arbeit schauen wir uns die ReLU
                    Funktion genauer an.</p>

                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 400 300">
                        <!-- Coordinate System -->
                        <line x1="50" y1="250" x2="350" y2="250" stroke="#000" stroke-width="2" />
                        <line x1="200" y1="50" x2="200" y2="280" stroke="#000" stroke-width="2" />
                        <!-- Y Axis in middle -->

                        <!-- ReLU Line -->
                        <!-- x < 0: y = 0 -->
                        <line x1="50" y1="250" x2="200" y2="250" stroke="#4d25fc" stroke-width="4" />
                        <!-- x > 0: y = x (approx, scale 1:1 visually) -->
                        <line x1="200" y1="250" x2="350" y2="100" stroke="#4d25fc" stroke-width="4" />

                        <text x="340" y="270" font-weight="bold">x</text>
                        <text x="210" y="60" font-weight="bold">y</text>
                        <text x="250" y="150" fill="#4d25fc" font-weight="bold" font-size="18">f(x) = max(0, x)</text>
                    </svg>
                </div>

                <p>Die ReLU-Funktion ergibt für alle Werte welche kleiner oder gleich null sind null. Bei positiven
                    Zahlen gibt die ReLU-Funktion die positive Zahl, ohne Veränderung, wieder zurück. Nach der
                    Ausführung der ReLU-Funktion kann abgelesen werden, welche Fragen zutreffen und welche nicht.
                    Sämtliche Werte, welche grösser als null sind, treffen zu. Alle, die gleich null sind, treffen nicht
                    zu.</p>
                <p>Bei dieser Funktion kommt nun wieder der Bias ins Spiel. Falls der Fragenvektor die Addition der
                    beiden Vektoren «Kann fliegen» und «Ist ein Flugzeug» bekommt man bereits einen positiven Wert, wenn
                    nur einer der beiden Fragen stimmt. Mit dem Bias gibt es die Möglichkeit, im Nachhinein das
                    Skalarprodukt noch anzupassen und zum Beispiel minus 1.9 rechnen. Da ein Skalarprodukt von zwei
                    Einheitsvektoren maximal eins ergibt, wenn diese genau übereinstimmen, würde dieser Bias bewirken,
                    dass unser Vektor aus der Attention extrem stark, mit den beiden Eigenschaften übereinstimmen muss.
                </p>

                <h4>Zweite Matrix Multiplikation</h4>
                <p>Aus der ReLU-Funktion erhalten wir einen Vektor mit Zahlen von null bis unendlich. Der nächste
                    Schritt innerhalb des MLP ist erneut eine Matrixmultiplikation wie im ersten Schritt. Bei dieser
                    Multiplikation ist es am besten, wenn man sich die Matrix spaltenweise vorstellt. Jede Spalte hat
                    genau die Grösse des ursprünglichen Vektorraums und widerspiegelt darin eine bestimmte Information.
                    In unserem Beispiel gäbe es innerhalb der Matrix zum Beispiel eine Spalte, welche die 150-187
                    Sitzplätze innerhalb des ursprünglichen Vektorraums widerspiegelt. Falls das Skalarprodukt der
                    Frage, ob es sich um einen Airbus A320 handelt, positiv ist, wird die ReLU Funktion den Wert nicht
                    anpassen und bei dieser Matrixmultiplikation wird der Vektor der Sitzplätze beim Endergebnis mit
                    einbezogen. Solange die ReLU-Funktion für eine Frage Null zurückgibt, wird diese Spalte nicht zum
                    Endergebnis dazugerechnet.</p>
                <p>Am Ende dieses Schrittes haben wir wieder einen Vektor, welcher die Grösse des ursprünglichen
                    Vektorenraums hat, und sämtliche zusätzlichen Informationen verglichen mit dem Vektor aus der
                    Attention enthält.</p>

                <h4>Residual Connection</h4>
                <p>Damit der ursprüngliche Vektor aus der Attention beim Weiteren verarbeiten nicht komplett vergessen
                    wird, addiert der letzte Schritt des MLP den Vektor aus der Attention mit dem Vektor, welcher die
                    zusätzlichen Fakten enthält.</p>

                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 150">
                        <defs>
                            <marker id="arrowhead-c" markerWidth="10" markerHeight="7" refX="9" refY="3.5"
                                orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4d25fc" />
                            </marker>
                        </defs>
                        <text x="50" y="80" text-anchor="middle" font-weight="bold">Attention Vektor</text>
                        <text x="250" y="80" text-anchor="middle" font-weight="bold">Fakten Vektor</text>
                        <text x="150" y="85" font-size="24">+</text>

                        <line x1="300" y1="80" x2="380" y2="80" stroke="#4d25fc" stroke-width="2"
                            marker-end="url(#arrowhead-c)" />

                        <text x="440" y="80" text-anchor="middle" font-weight="bold" fill="#a777d1">Ergebnis</text>
                    </svg>
                </div>

                <p>In unserem Beispiel hätten wir nun einen Vektor, welcher Airbus, A320 und nun auch die 150-187
                    Sitzplätze beinhaltet.</p>
            </div>

            <!-- Training Section -->
            <div class="tip-box">
                <h3>Erklärung Training</h3>
                <p>Wenn man bei einem neuronalen Netzwerk «lernt» sagt, meint man damit eigentlich nichts anderes, als
                    die Parameter zu ändern, sodass man ein gewünschtes Ergebnis bekommt. Dies ist jedoch nicht trivial,
                    denn heutige LLMs haben über eine Billion Parameter. Man kann sich diese wie viele Schalter
                    vorstellen, die man beliebig drehen kann, um einen anderen Wert zu erhalten.</p>

                <h4>Backpropagation</h4>
                <p>Backpropagation ist der zentrale Algorithmus, mit dem ein neuronales Netzwerk lernt.</p>
                <p>Vereinfacht funktioniert der Algorithmus so, dass man einen Satz hat, den man kennt.</p>
                <p>Als Beispiel nehmen wir: «Ein Kirschbaum ist pink.» Jetzt füttern wir das Netzwerk mit diesem Satz,
                    lassen jedoch das letzte Wort weg. Das heisst wir fragen das Netzwerk, was das nächste Wort im Satz
                    «ein Kirschbaum ist» ist.</p>
                <p>Das Netzwerk wird jetzt durch die Schritte gehen, also Embedding, Attention, MLP und so weiter. Am
                    Ende wird es eine Vorhersage machen, die könnte wie folgt aussehen:</p>
                <ul>
                    <li>45% pink</li>
                    <li>20% gross</li>
                    <li>20% wunderschön</li>
                    <li>15% weiss</li>
                </ul>
                <p>Das Netzwerk hat «pink» mit 45% als wahrscheinlichstes nächstes Wort gewählt – Das ist richtig,
                    jedoch mit 45% nicht überzeugend. Der Verlust misst nun, wie weit die Vorhersage von der Wahrheit
                    entfernt ist. Mathematisch nutzen wir dafür die Cross-Entropy: Je höher die vorhergesagte
                    Wahrscheinlichkeit für das richtige Wort, desto kleiner der Verlust. In unserem Fall ist die
                    Wahrscheinlichkeit für «pink» 45%, also ist der Verlust deutlich grösser als null, in unserem
                    Beispiel 55%.</p>

                <!-- Visualization: Prediction vs Target Loss -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 250">
                        <text x="100" y="30" text-anchor="middle" font-weight="bold">Vorhersage</text>
                        <text x="300" y="30" text-anchor="middle" font-weight="bold">Ziel (Wahrheit)</text>
                        <text x="450" y="30" text-anchor="middle" font-weight="bold" fill="#ff4d4d">Verlust</text>

                        <!-- Prediction Bar -->
                        <rect x="50" y="50" width="100" height="20" fill="#a777d1" opacity="0.45" /> <text x="160"
                            y="65">pink (45%)</text>
                        <rect x="50" y="80" width="100" height="20" fill="#ccc" opacity="0.2" /> <text x="160"
                            y="95">gross (20%)</text>
                        <rect x="50" y="110" width="100" height="20" fill="#ccc" opacity="0.2" /> <text x="160"
                            y="125">schön (20%)</text>
                        <rect x="50" y="140" width="100" height="20" fill="#ccc" opacity="0.15" /> <text x="160"
                            y="155">weiss (15%)</text>

                        <!-- Target Bar -->
                        <rect x="250" y="50" width="100" height="20" fill="#4d25fc" /> <text x="360" y="65">pink
                            (100%)</text>
                        <rect x="250" y="80" width="100" height="20" fill="none" stroke="#ccc" />
                        <rect x="250" y="110" width="100" height="20" fill="none" stroke="#ccc" />
                        <rect x="250" y="140" width="100" height="20" fill="none" stroke="#ccc" />

                        <!-- Loss Arrow -->
                        <line x1="300" y1="180" x2="100" y2="180" stroke="#ff4d4d" stroke-width="2"
                            marker-end="url(#arrowhead)" />
                        <text x="200" y="200" text-anchor="middle" fill="#ff4d4d">Differenz = Hoher Loss</text>
                    </svg>
                </div>

                <p>Das Ziel: Diesen Verlust minimieren.</p>

                <h4>Lernen im Detail</h4>
                <p>Doch wie funktioniert das Lernen im Detail? Um das zu verstehen, hilft ein einfacheres Beispiel.
                    Stell dir ein winziges neuronales Netzwerk vor mit nur drei Schichten: zehn Input-Neuronen für die
                    zehn häufigsten Wörter, vier Neuronen in einer versteckten Schicht und zwei Output-Neuronen für die
                    Kategorien «Säugetier» und «Vogel».</p>

                <!-- Visualization: Small Network -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 200">
                        <text x="50" y="30" text-anchor="middle" font-size="12">Input (Wörter)</text>
                        <text x="250" y="30" text-anchor="middle" font-size="12">Hidden</text>
                        <text x="450" y="30" text-anchor="middle" font-size="12">Output (Kategorie)</text>

                        <!-- Input Nodes -->
                        <circle cx="50" cy="60" r="10" fill="#fff" stroke="#4d25fc" />
                        <circle cx="50" cy="90" r="10" fill="#fff" stroke="#4d25fc" />
                        <circle cx="50" cy="120" r="10" fill="#fff" stroke="#4d25fc" />
                        <text x="50" y="150" text-anchor="middle">...</text>
                        <circle cx="50" cy="170" r="10" fill="#fff" stroke="#4d25fc" />

                        <!-- Hidden Nodes -->
                        <circle cx="250" cy="70" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                        <circle cx="250" cy="110" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                        <circle cx="250" cy="150" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                        <circle cx="250" cy="190" r="15" fill="#e6e0f8" stroke="#4d25fc" />

                        <!-- Output Nodes -->
                        <circle cx="450" cy="80" r="20" fill="#fff" stroke="#4d25fc" /> <text x="450" y="85"
                            text-anchor="middle" font-size="10">Säugetier</text>
                        <circle cx="450" cy="160" r="20" fill="#fff" stroke="#4d25fc" /> <text x="450" y="165"
                            text-anchor="middle" font-size="10">Vogel</text>

                        <!-- Connections (simplified) -->
                        <line x1="60" y1="60" x2="235" y2="70" stroke="#ccc" stroke-width="1" />
                        <line x1="60" y1="90" x2="235" y2="110" stroke="#ccc" stroke-width="1" />
                        <line x1="265" y1="70" x2="430" y2="80" stroke="#ccc" stroke-width="1" />
                        <line x1="265" y1="190" x2="430" y2="160" stroke="#ccc" stroke-width="1" />
                    </svg>
                </div>

                <p>Wir trainieren es damit, das Wort «Katze» als Säugetier zu klassifizieren. Im Forward-Pass fliesst
                    die Aktivierung durch das Netz, am Ende kommt bei «Säugetier» ein Wert von 0,7 heraus – richtig,
                    aber unsicher. Der Verlust beträgt -log(0,7) = 0,15.</p>

                <!-- Visualization: Network with values -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 200">
                        <!-- Same structure, added predictions -->
                        <text x="50" y="30" text-anchor="middle" font-size="12">Input "Katze"</text>
                        <!-- Input Active -->
                        <circle cx="50" cy="90" r="10" fill="#4d25fc" stroke="#4d25fc" />

                        <!-- Hidden Processing -->
                        <circle cx="250" cy="70" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                        <circle cx="250" cy="110" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                        <circle cx="250" cy="150" r="15" fill="#e6e0f8" stroke="#4d25fc" />

                        <!-- Output with Values -->
                        <circle cx="450" cy="80" r="20" fill="#a777d1" stroke="#4d25fc" />
                        <text x="490" y="85" font-weight="bold">0.7</text>
                        <text x="450" y="60" text-anchor="middle" font-size="10">Säugetier</text>

                        <circle cx="450" cy="160" r="20" fill="#fff" stroke="#4d25fc" />
                        <text x="490" y="165" font-weight="bold">0.3</text>
                        <text x="450" y="140" text-anchor="middle" font-size="10">Vogel</text>

                        <!-- Loss -->
                        <rect x="380" y="180" width="120" height="30" rx="5" fill="#ff4d4d" opacity="0.2" />
                        <text x="440" y="200" text-anchor="middle" fill="#d00" font-weight="bold">Loss = 0.15</text>
                    </svg>
                </div>

                <p>Jetzt beginnt der Backward-Pass: Dieser Fehler von 0,15 wandert rückwärts durch jede einzelne
                    Verbindung. Für jedes Gewicht berechnen wir, wie stark es zum Fehler beigetragen hat, also wie sehr
                    eine Änderung an diesem Gewicht das Endergebnis ändern würde.</p>
                <p>Die Kettenregel ist das mathematische Rückgrat des Backward-Passes. Sie funktioniert wie eine
                    Abstiegsreise mit Zwischenstationen: Der Gesamtfehler am Ende (0,15) hängt vom Output-Wert ab, der
                    wiederum vom vorletzten Gewicht abhängt, dieses vom davorliegenden Neuron, und so weiter bis zum
                    ersten Gewicht am Input. Die Kettenregel sagt nun: Um zu wissen, wie sehr sich der Fehler ändert,
                    wenn wir ein einzelnes Gewicht am Anfang verändern, multiplizieren wir die Ableitungen aller
                    Zwischenstationen miteinander.</p>
                <p>Konkret im Netzwerk: Der Fehler von 0,15 hängt direkt vom Output-Wert (0,7) ab. Der Output-Wert hängt
                    von der Summe der Hidden-Neuronen ab, die wiederum von den Input-Gewichten für «Katze» abhängen. Die
                    Kettenregel multipliziert nun für jede Verbindung die lokalen Ableitungen – also: Wie stark ändert
                    sich die Aktivierung dieser Schicht, wenn ich das Gewicht davor verändere? Durch das geschickte
                    Zwischenspeichern dieser Teilableitungen während des Forward-Passes kann der Algorithmus im
                    Backward-Pass in einem einzigen Sweep von hinten nach vorne alle Gradienten berechnen, ohne für
                    jedes Gewicht einen separaten Durchlauf zu starten.</p>

                <!-- Visualization: Backpropagation Arrows -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 500 200">
                        <defs>
                            <marker id="arrowhead-red" markerWidth="10" markerHeight="7" refX="9" refY="3.5"
                                orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#ff4d4d" />
                            </marker>
                        </defs>
                        <!-- Nodes lighter -->
                        <g opacity="0.3">
                            <circle cx="50" cy="90" r="10" fill="#4d25fc" />
                            <circle cx="250" cy="70" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                            <circle cx="250" cy="110" r="15" fill="#e6e0f8" stroke="#4d25fc" />
                            <circle cx="450" cy="80" r="20" fill="#a777d1" stroke="#4d25fc" />
                        </g>

                        <!-- Backward arrows -->
                        <line x1="430" y1="80" x2="270" y2="70" stroke="#ff4d4d" stroke-width="4"
                            marker-end="url(#arrowhead-red)" />
                        <line x1="430" y1="80" x2="270" y2="110" stroke="#ff4d4d" stroke-width="2"
                            marker-end="url(#arrowhead-red)" />

                        <line x1="235" y1="70" x2="65" y2="90" stroke="#ff4d4d" stroke-width="3"
                            marker-end="url(#arrowhead-red)" />

                        <text x="250" y="180" text-anchor="middle" fill="#ff4d4d" font-weight="bold">Fehler wird
                            zurückgerechnet</text>
                    </svg>
                </div>

                <p>Die Gradienten zeigen die Richtung des steilsten Anstiegs. Wir wollen aber den Fehler minimieren,
                    also gehen wir in die entgegengesetzte Richtung. Die Lernrate, typischerweise eine kleine Zahl wie
                    0,01, bestimmt die Schrittgrösse:</p>
                <p><code>Neues Gewicht = altes Gewicht - Lernrate * Gradient</code></p>
                <p>Ein Gewicht, das den Fehler vergrössert hat, wird verringert; eines, das ihn verringern würde, wird
                    erhöht. Dies passiert für alle Parameter gleichzeitig.</p>

                <!-- Visualization: Weight Update Zoom -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 400 150">
                        <circle cx="50" cy="75" r="20" fill="#fff" stroke="#4d25fc" />
                        <circle cx="350" cy="75" r="20" fill="#fff" stroke="#4d25fc" />
                        <line x1="70" y1="75" x2="330" y2="75" stroke="#ccc" stroke-width="2" />

                        <!-- Weight control -->
                        <circle cx="200" cy="75" r="15" fill="#a777d1" />
                        <text x="200" y="110" text-anchor="middle" font-size="12">Gewicht w</text>

                        <!-- Adjustment -->
                        <path d="M 180 50 Q 200 30 220 50" fill="none" stroke="#ff4d4d" stroke-width="2"
                            marker-end="url(#arrowhead-red)" />
                        <text x="200" y="25" text-anchor="middle" fill="#ff4d4d" font-size="12">- Update</text>
                    </svg>
                </div>
            </div>

            <div class="tip-box">
                <h3>Wie jedoch lernen LLMs?</h3>
                <p>Dieses Prinzip, so einfach es klingt, ist exakt das, was in LLMs passiert. Statt zehn Gewichten hat
                    GPT-3 175 Milliarden Parameter. Statt drei Schichten hat es 96 Transformer-Blöcke. Statt eines
                    einzelnen «Katze»-Beispiels trainiert es gleichzeitig über Tausende von Sätzen. Der Ablauf bleibt
                    1:1 identisch: Forward-Pass durch Embedding, Attention und MLP, Berechnung des Verlustes mit
                    Cross-Entropy, Backward-Pass durch alle 96 Schichten hinunter zu den Embeddings, und schliesslich
                    das Update aller Parameter.</p>

                <!-- Visualization: Comparison -->
                <div class="nn-architecture-diagram">
                    <svg viewBox="0 0 600 300">
                        <!-- Small Net -->
                        <g transform="translate(50, 50)">
                            <rect width="100" height="200" fill="none" stroke="#ccc" stroke-dasharray="5,5" />
                            <text x="50" y="-10" text-anchor="middle" font-weight="bold">Unser Beispiel</text>

                            <circle cx="50" cy="30" r="10" fill="#fff" stroke="#4d25fc" />
                            <circle cx="50" cy="100" r="10" fill="#e6e0f8" stroke="#4d25fc" />
                            <circle cx="50" cy="170" r="10" fill="#fff" stroke="#4d25fc" />
                            <line x1="50" y1="40" x2="50" y2="90" stroke="#ccc" />
                            <line x1="50" y1="110" x2="50" y2="160" stroke="#ccc" />
                            <text x="50" y="220" text-anchor="middle" font-size="12">~20 Parameter</text>
                        </g>

                        <!-- Transformer Tower -->
                        <g transform="translate(300, 50)">
                            <text x="100" y="-10" text-anchor="middle" font-weight="bold">GPT-3 (LLM)</text>
                            <!-- Stack of blocks -->
                            <rect x="50" y="180" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />
                            <rect x="50" y="155" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />
                            <rect x="50" y="130" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />
                            <rect x="50" y="105" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />
                            <text x="100" y="90" text-anchor="middle" font-weight="bold">...</text>
                            <rect x="50" y="30" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />
                            <rect x="50" y="5" width="100" height="20" fill="#e6e0f8" stroke="#4d25fc" />

                            <text x="200" y="100" font-size="12">96 Layer</text>
                            <text x="100" y="220" text-anchor="middle" font-size="12">175'000'000'000 Parameter</text>
                        </g>
                    </svg>
                </div>

                <p>Was bei LLMs jedoch besonders ist: Die Embedding-Matrix selbst lernt mit. Während des Trainings
                    rücken «Kirschbaum» und «Baum» im Vektorraum näher zusammen, während «pink» eine spezielle Beziehung
                    zu «Kirschbaum» entwickelt. Die Attention-Parameter lernen, welche Wörter im Satz wichtig sind. Und
                    weil die oberen Schichten direkt für die Vorhersage verantwortlich sind, lernen sie schneller als
                    die unteren, die allgemeine Sprachmuster erfassen.</p>
                <p>Dieser Zyklus wird nicht einmal oder hundert Mal durchgeführt, sondern Milliarden Mal. Jede Epoche,
                    also jeder Durchlauf durch das gesamte Training, feilt die Parameter ein wenig mehr. Irgendwann wird
                    aus dem 45%-Vorschlag für «pink» eine 99%-Wahrscheinlichkeit. Das Netzwerk hat gelernt, dass
                    Kirschbäume pink sein können – nicht durch Regeln, sondern durch reine Optimierung von Zahlen.</p>
                <p>Backpropagation ist also kein magischer Lernprozess, sondern ein mathematischer
                    Fehlerkorrekturmechanismus, der durch die Kettenregel der Analysis effizient Millionen von
                    Parametern gleichzeitig steuert. Ob zehn Gewichte oder 100 Milliarden – das Prinzip bleibt: Fehler
                    messen, ableiten, anpassen und wiederholen.</p>
            </div>

            <div class="tip-box">
                <h3>Reinforcement Learning from Human Feedback</h3>
                <p>RLHF ist eine Training Methode, bei welcher Menschen die Antwort zu einem gegebenen Prompt schreiben.
                    Anhand dieser Prompts und Antworten wird dann das LLM trainiert. Damit erhält man ein LLM, welches
                    unterandere auch moralische Werte abbilden kann. So kann das LLM zum Beispiel lernen, dass Gewalt
                    keine Lösung für einen verbalen Konflikt ist, wobei, rein faktisch gesehen, Gewalt eine Lösung ist.
                    Diese Trainings Methode ist zeitintensiv und teuer, da viel manuelle Arbeit ins Trainieren
                    einfliesst.</p>
                <p>Um Kosten und Zeit zu sparen, wird nach dem erfolgreichen ersten Training, wie oben beschrieben, das
                    LLM nach einem anderen Vorgehen weitertrainiert. Anstelle, dass das LLM anhand bereits bestehender
                    Antworten trainiert wird, generiert es nun mehrere Antworte auf einen Prompt selbständig. Menschen
                    bewerten dann die generierten Antworten von der besten zur schlechtesten. Das LLM kann dann anhand
                    der unterschiedlichen Bewertungen seine Modelparameter noch feiner einstellen. Durch dieses Vorgehen
                    konnte schon Zeit gespart werden, weiterhin muss jedoch ein Mensch aktiv sämtliche Antworten
                    bewerten.</p>
            </div>

            <div class="tip-box">
                <h3>Zusammenfassung</h3>
                <h4>Grundprinzip: Next Word Prediction</h4>
                <p>LLMs fungieren im Kern als komplexe probabilistische Systeme. Ihre primäre Aufgabe ist die Next Word
                    Prediction: Basierend auf einer gegebenen Eingabe (Prompt) berechnet das Modell die bedingte
                    Wahrscheinlichkeit für das nachfolgende Token.</p>
                <p>Um repetitive oder deterministische Ausgaben zu vermeiden und eine natürlichere Sprachgenerierung zu
                    ermöglichen, wählt das Modell nicht zwingend das Token mit der höchsten Wahrscheinlichkeit, sondern
                    nutzt Sampling-Verfahren zur Varianzbildung. Dieser Prozess verläuft in einer Schleife, heisst,
                    jedes generierte Output-Token wird unmittelbar in die Eingabesequenz für die nächste Berechnung
                    integriert.</p>

                <h4>Datenrepräsentationen: Vektorisierung und Embedding</h4>
                <p>Da neuronale Netze mathematische Operationen ausführen, ist eine Transformation sprachlicher Eingaben
                    in numerische Werte erforderlich. Dies geschieht durch sogenannte Embeddings.</p>
                <p>Wörter werden hier in hochdimensionale Vektoren übersetzt und in einem Vektorraum positioniert.
                    Semantisch verwandte oder kontextuell ähnliche Begriffe (z.B. Synonyme) weisen in diesem
                    mathematischen Raum eine geringe Distanz zueinander auf. Diese räumliche Nähe ermöglicht es dem
                    Modell, semantische Beziehungen zu erfassen.</p>

                <h4>Die Transformer-Architektur</h4>
                <p>Moderne LLMs basieren auf der Transformer-Architektur, welche sich durch zwei Hauptmechanismen
                    auszeichnet:</p>
                <ul>
                    <li><strong>Attention:</strong> Dieser Mechanismus ermöglicht dem Modell die Kontextualisierung von
                        Informationen. Über Query- und Key-Vektoren wird mit dem Skalarprodukt die Relevanz zwischen
                        einzelnen Tokens berechnet. Ein hoher Skalar indiziert eine starke semantische Beziehung.</li>
                    <li><strong>MLP:</strong> Nach der Kontextanalyse durch die Attention-Layer werden die Informationen
                        im MLP weiterverarbeitet. Dieser Teil des Netzwerks dient primär als faktischer Wissensspeicher.
                        Durch Matrixmultiplikationen werden kontextuelle Informationen mit gelerntem Weltwissen
                        verknüpft und in den Ergebnisvektor integriert.</li>
                </ul>

                <h4>Training: Optimierung durch Backpropagation</h4>
                <p>Der Lernprozess eines LLMs entspricht der ständigen Optimierung seiner Gewichte. Das Verfahren nennt
                    man hier die Backpropagation:</p>
                <ul>
                    <li><strong>Forward Pass:</strong> Das Modell generiert eine Vorhersage.</li>
                    <li><strong>Loss Calculation:</strong> Das System vergleicht die Vorhersage des Modells mit der
                        tatsächlich korrekten Antwort.</li>
                    <li><strong>Backward Pass:</strong> Der berechnete Fehler wird nun rückwärts durch das Netzwerk
                        geleitet. Dabei analysiert der Algorithmus, welche Gewichte massgeblich zu diesem Fehler
                        beigetragen haben. Die Abweichung zwischen beiden wird als "Loss" berechnet.</li>
                    <li><strong>Parameter Update:</strong> Die Gewichte werden minimal angepasst. Das Ziel ist es, die
                        Einstellungen so zu verändern, dass das Modell bei einer ähnlichen Aufgabe in Zukunft einen
                        geringeren Fehler macht.</li>
                </ul>
            </div>
        </section>
    </main>

    <div id="footer-placeholder"></div>

    <script src="/js/main.js"></script>
    <script src="/js/footer.js"></script>
    <script src="/js/neural_network.js"></script>
</body>

</html>