<!DOCTYPE html>
<html lang="de">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDPA FLMI - LLM</title>
    <link rel="stylesheet" href="/style.css">
</head>

<body>
    <div id="header-placeholder"></div>

    <main>
        <section class="hero">
            <h1 class="gradient-text">LLM</h1>
            <p>In diesem Abschnitt wird beschrieben, wie ein LLM funktioniert.</p>
            <a href="#neurale-netzwerke" class="button">Mehr erfahren</a>
        </section>

        <section id="neurale-netzwerke">
            <h2 class="gradient-text">Neurale Netzwerke</h2>
            <h3 id="grundlagen">Grundlagen</h3>
            <p>Ein künstliches neuronales Netzwerk (KNN) ist der Versuch, die Funktionsweise des menschlichen
                Gehirns mathematisch nachzubauen, um Computer dazu zu bringen, komplexe Muster zu erkennen.
                Während klassische Computerprogramme stur einer programmierten Logik folgen (Wenn A, dann B),
                lernt ein neuronales Netzwerk aus Erfahrungen, ähnlich wie ein Kind lernt, eine Katze von einem
                Hund zu unterscheiden.</p>

            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="architecture-container"
                        style="display: grid; grid-template-columns: repeat(4, 1fr); position: relative;">
                        <!-- Connection Layer (SVG) -->
                        <svg class="connection-layer"
                            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 0; pointer-events: none;"
                            preserveAspectRatio="none">
                            <defs>
                                <linearGradient id="lineGraph" x1="0%" y1="0%" x2="100%" y2="0%">
                                    <stop offset="0%" style="stop-color:#cccccc;stop-opacity:0.6" />
                                    <stop offset="100%" style="stop-color:#cccccc;stop-opacity:0.6" />
                                </linearGradient>
                            </defs>
                            <!-- Input (3) to Hidden 1 (3) - Centers at roughly 12.5% and 37.5% -->
                            <!-- y percentages based on 3 nodes (25%, 50%, 75%) approx -->
                            <line x1="12.5%" y1="25%" x2="37.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="25%" x2="37.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="25%" x2="37.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <line x1="12.5%" y1="50%" x2="37.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="50%" x2="37.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="50%" x2="37.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <line x1="12.5%" y1="75%" x2="37.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="75%" x2="37.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="12.5%" y1="75%" x2="37.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <!-- Hidden 1 (3) to Hidden 2 (3) - Centers at 37.5% and 62.5% -->
                            <line x1="37.5%" y1="25%" x2="62.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="25%" x2="62.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="25%" x2="62.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <line x1="37.5%" y1="50%" x2="62.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="50%" x2="62.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="50%" x2="62.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <line x1="37.5%" y1="75%" x2="62.5%" y2="25%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="75%" x2="62.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="37.5%" y1="75%" x2="62.5%" y2="75%" stroke="#ccc" stroke-width="1" />

                            <!-- Hidden 2 (3) to Output (1) - Centers at 62.5% and 87.5% -->
                            <!-- Output node at 50% vertical? -->
                            <line x1="62.5%" y1="25%" x2="87.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="62.5%" y1="50%" x2="87.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                            <line x1="62.5%" y1="75%" x2="87.5%" y2="50%" stroke="#ccc" stroke-width="1" />
                        </svg>

                        <!-- Input Layer -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <div class="node circle input"></div>
                            <div class="node circle input"></div>
                            <div class="node circle input"></div>
                            <span class="layer-label">Input Layer</span>
                        </div>

                        <!-- Hidden Layer 1 -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <div class="node circle hidden"></div>
                            <div class="node circle hidden"></div>
                            <div class="node circle hidden"></div>
                        </div>

                        <!-- Hidden Layer 2 -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <div class="node circle hidden"></div>
                            <div class="node circle hidden"></div>
                            <div class="node circle hidden"></div>
                            <span class="layer-label">Hidden Layers</span>
                        </div>

                        <!-- Output Layer -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <div class="node circle output"></div>
                            <span class="layer-label">Output Layer</span>
                        </div>
                    </div>
                </div>
            </div>

            <div>
                <h3>Gewichte und Parameter</h3>
                <p>Das wichtigste Konzept zum Verständnis sind die Verbindungen zwischen den Neuronen. Jedes
                    Neuron ist mit den Neuronen der nächsten Schicht verbunden. Diese Verbindungen sind jedoch
                    nicht alle gleich stark. Jede Verbindung hat ein sogenanntes Gewicht (Weight).</p>
                <p>Man kann sich diese Gewichte wie Wasserleitungen mit Ventilen vorstellen: Bei manchen
                    Verbindungen ist das Ventil weit offen (das Signal kommt stark durch), bei anderen ist es
                    fast zu (das Signal wird blockiert).</p>
            </div>

            <!-- Interactive Valve Element -->
            <div class="valve-interactive-container">
                <h4>Interaktives Element: Das Ventil</h4>
                <div class="valve-visual">
                    <div class="valve-node">
                        Input
                        <span class="valve-value-label" id="valve-input-value">1.0</span>
                    </div>
                    <div class="valve-connection-wrapper">
                        <span class="valve-value-label valve-weight-label" id="valve-weight-value">Weight:
                            1.0</span>
                        <div id="valve-connection"></div>
                    </div>
                    <div class="valve-node">
                        Output
                        <span class="valve-value-label" id="valve-output-value">1.0</span>
                    </div>
                </div>
                <div class="valve-controls">
                    <input type="range" id="valve-slider" class="valve-slider" min="-5" max="5" step="0.1" value="1">
                    <p>Stell dir das Gewicht als Ventil vor. Verändere das Gewicht, um zu sehen, wie viel vom
                        Input beim Output ankommt.</p>
                </div>
            </div>

            <p>Zusammengenommen bilden diese Gewichte und Einstellungen die Parameter des Modells. Wenn wir
                später davon sprechen, dass ein LLM «Milliarden von Parametern» hat, meinen wir genau diese
                Milliarden von kleinen Stellschrauben (unseren Ventilen) und Verbindungen innerhalb des
                neuronalen Netzwerks, die den Informationsfluss steuern.</p>

            <p>Das Ziel des Netzwerks ist es, diese Parameter so einzustellen, dass am Ende das richtige
                Ergebnis herauskommt. Ein LLM ist im Grunde nichts anderes als ein gigantisches, hochkomplexes
                neuronales Netzwerk mit einer speziellen Architektur, die sich besonders gut für Sprache eignet.
            </p>

            <div>
                <h3>Die Mathematik hinter einem Neuron</h3>
                <p>Um zu verstehen, wie das Netzwerk "denkt", müssen wir uns ansehen, was in einem einzelnen
                    Neuron mathematisch passiert. Es ist weniger kompliziert, als es aussieht. Der Prozess in
                    einem einzelnen Neuron lässt sich in zwei Schritte unterteilen: die lineare Funktion und die
                    Aktivierungsfunktion.</p>

                <h4>1. Die gewichtete Summe (Linearer Teil)</h4>
                <p>Anstatt abstrakter Formeln können wir uns dies visuell vorstellen. Ein Neuron sammelt Informationen
                    von seinen Vorgängern. Jede Verbindung gewichtet die Information unterschiedlich stark – wie ein
                    Ventil, das mehr oder weniger Wasser durchlässt.</p>



                <p>Mathematisch dargestellt: <code>z = (x1 · w1) + ... + b</code>. Dies ist der "Input" für den nächsten
                    Schritt, die Aktivierungsfunktion.</p>

                <h4>2. Die Aktivierungsfunktion (Nicht-linearer Teil)</h4>
                <p>Nachdem die gewichtete Summe (z) berechnet wurde, ist das Ergebnis oft eine beliebige Zahl
                    zwischen minus unendlich und plus unendlich. Damit das Netzwerk komplexe Probleme lösen
                    kann, muss entschieden werden: "Ist dieses Signal stark genug, um weitergeleitet zu
                    werden?".</p>
                <p>Hier kommt die Aktivierungsfunktion (f) ins Spiel. Sie nimmt das Ergebnis der Summe und
                    wandelt es um:</p>
                <p
                    style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                    output = f(z)</p>
                <p>In modernen LLMs wird hier oft die ReLU-Funktion (Rectified Linear Unit) verwendet. Sie ist
                    mathematisch sehr simpel: Sie wandelt alle negativen Zahlen in Null um.</p>
                <p
                    style="font-family: monospace; background: #eee; padding: 10px; border-radius: 5px; display: inline-block;">
                    f(z) = max(0, z)</p>
                <p>Das bedeutet: Wenn die gewichtete Summe negativ ist, bleibt das Neuron "still" (Output 0).
                    Ist sie positiv, leitet es das Signal weiter. Ohne diese nicht-linearen Funktionen wäre das
                    gesamte Netzwerk, egal wie gross, nur eine einfache lineare Regression und könnte keine
                    Sprache verstehen.</p>
            </div>
        </section>
        </section>

        <section id="llm-aufbau">
            <h2 class="gradient-text">LLM Aufbau</h2>
            <h3 id="architekturen">Architekturen</h3>
            <p>Ein LLM kann sehr unterschiedlich aufgebaut sein. Die bekanntesten LLM-Architekturen heissen
                Transformer, Mamba und RNN (Rekurrente Neurale Netze). RRN wurden durch Transformer abgelöst, dabei
                wurden zwei grosse Probleme behoben. Transformer laufen im Vergleich zu RRN parallel, somit können
                sie einen Text gelichzeitig abarbeiten und gehen nicht Wort für Wort vor. Zusätzlich können sich
                Transformer an sämtliche Informationen im Text erinnern, während RRN die Informationen, wenn der
                Text zu lange war, nach und nach wieder vergassen. Die Transformer Architektur ist aktuell die
                meistverbreitete Architektur und wird in ChatGPT, Copilot, Gemini und vielen weiteren Chatbots
                verwendet. Mamba ist eine Architektur, die gerade erst entwickelt wird. Hier wird versucht den
                Rechenaufwand, der bei Transformer quadratisch zur Textlänge steigt, linear zur Textlänge gestalten.
                Innerhalb dieser IDPA werden wir nur auf die Transformer Architektur weiter eingehen, da diese
                aktuell am meisten verwendet wird.</p>
            <p>Bei allen folgenden Informationen zum Transformer Aufbau muss beachtet werden, dass diese stark
                vereinfacht sind. Da LLM ihre Modell-Parameter selbst einstellen, kann zwar erklärt werden, welche
                Mathematische Funktion ausgeführt werden, was diese aber schlussendlich wirklich bewirken, bleibt
                selbst für Expert:innen oft eine Vermutung.</p>

            <p>Grundsätzlich ist ein Transformer in vier Teilschritte trennbar. Im ersten Teil wandelt der
                Transformer die, für uns Menschen verständliche, Wörter aus der Eingabe in Computer verständliche
                Worte um. Dieser Schritt wird Embedding genannt. Anschliessend in der sogenannten Attention
                berechnet der Transformer den Kontext der einzelnen Worte. Dieser Schritt ermöglicht es, komplexere
                Wörter oder Sätze zu verstehen. So gibt es zum Beispiel Wörter, welche mehrere Bedeutungen haben,
                wobei die richtige Bedeutung erst durch den Kontext hervorkommt. Ein Beispiel für ein solches Wort
                ist die “Bank”. Im nächsten Schritt hat die LLM dann noch die Chance das Wort, welches generiert
                wird, mit eigenen Informationen anzureichern. Bei diesem Schritt gibt es verschiedene
                Lösungsansätze, wir schauen uns nur das MLP (Multilayer Perzeptron) genauer an. Nach diesem Schritt
                ist die generierung des Wortes abgeschlossen und das Wort wird wieder umgewandelt in ein, für uns
                Menschen, verständliches Wort.</p>
        </section>

        <section id="embedding">
            <h2 class="gradient-text">Embedding</h2>
            <h3 id="vektoren">Bedeutung in Vektoren</h3>
            <p>Ein Wort erhält im Sprachmodell seine Bedeutung nicht in sich selbst, sondern aus den vielen
                Beziehungen, die es zu anderen Wörtern hat. Die Vektoreneinbettung ist das Zuteilen der Bedeutung zu
                Wörtern, indem es gegen andere ähnliche Wörter verglichen wird. Sie erhalten einen eigenen Wert, der
                dann durch den Kontext anderer Wörter in einem Satz ergänzt wird.</p>
            <p>Wie kann nun die Bedeutung eines Wortes in Werten festgehalten werden? In LLMs wird dies mit
                hochdimensionalen Vektoren gemacht.</p>
            <p>Ein Beispiel für die Festhaltung von nicht numerischer Bedeutung in Vektoren ist wie Computer mit
                Farben im RGB-Profil umgehen. Diese werden in einem 3-dimensionalen Vektor festgehalten.</p>
            <p>Wir haben 3 Dimensionen, drei «Richtungen». Rot, Grün und Blau. Die Werte gehen von 0-255. Der
                Computer versteht nicht, dass dies die Farbe Rot ist, doch er weiss, wo Rot im 3-dimensionalen Raum
                liegt. (255, 0, 0) bedeutet “Die Roten Pixel sollen auf der höchsten Helligkeit sein”.</p>

            <div class="tip-box-special">
                <h4>Interaktives Element: Farben als Vektoren (3D)</h4>
                <!-- 3D Canvas Container -->
                <div id="rgb-cube-container"
                    style="width: 100%; height: 400px; background-color: #E5E7EB; border-radius: 10px; margin-bottom: 20px; position: relative;">
                    <p style="text-align:center; padding-top: 180px; color: #999;">Lade 3D Ansicht...</p>
                </div>

                <div class="rgb-controls">
                    <div class="rgb-slider-row">
                        <span class="rgb-label" style="color: #ff0000;">Rot (X)</span>
                        <input type="range" class="rgb-slider-input" id="slider-r" min="0" max="255" value="0">
                        <span class="rgb-value" id="val-r">0</span>
                    </div>
                    <div class="rgb-slider-row">
                        <span class="rgb-label" style="color: #00ff00;">Grün (Y)</span>
                        <input type="range" class="rgb-slider-input" id="slider-g" min="0" max="255" value="0">
                        <span class="rgb-value" id="val-g">0</span>
                    </div>
                    <div class="rgb-slider-row">
                        <span class="rgb-label" style="color: #0000ff;">Blau (Z)</span>
                        <input type="range" class="rgb-slider-input" id="slider-b" min="0" max="255" value="0">
                        <span class="rgb-value" id="val-b">0</span>
                    </div>
                </div>
                <div class="rgb-vector-display">
                    Vektor: (<span id="vec-r">0</span>, <span id="vec-g">0</span>, <span id="vec-b">0</span>)
                </div>
                <p style="margin-top: 20px; font-size: 0.9em; font-style: italic;">Der Vektor zeigt vom Ursprung
                    (Schwarz) zur gemischten Farbe im RGB-Würfel. Rotieren & Zoomen möglich.</p>
            </div>

            <!-- Import Map for Three.js -->
            <script type="importmap">
                    {
                        "imports": {
                            "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                            "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
                        }
                    }
                </script>
            <!-- 3D Logic Script -->
            <script type="module" src="/js/rgb_cube.js?v=2"
                onerror="document.getElementById('rgb-3d-container').innerHTML = '<p style=\'padding:20px; color:red\'>Fehler beim Laden von 3D. Bitte Internetverbindung prüfen.</p>'"></script>

            <p>Ein Computer versteht so welche Farbe es anzeigen soll, somit “Weiss” der Computer, was Blau ist, und
                kann es anzeigen. Er versteht sogar den Unterschied zwischen Hellblau und Dunkelblau, da der Vektor
                der Farbe einfach weiter Richtung weiss zeigt. Er weiss nicht, was Rot wirklich ist, doch kennt
                seinen Vektor. So wird eine Bedeutung in einem Vektor festgehalten.</p>
            <p>Wörter sind für LLMs genauso Vektoren, nur dass diese durch die erhöhte Komplexität der Bedeutung von
                Wörtern deutlich mehr Dimensionen haben, welche dem LLM Helfen “Attribute” der Wörter festzuhalten.
                Bei modernen LLMs sind dies ungefähr 2’000-10’000 Dimensionen. Diese Dimensionen sind nicht benannt
                wie in unserem Beispiel der Farben (Richtung Rot, Grün und Blau), doch das LLM weiss was diese
                bedeuten.</p>
            <p>Die Wörter müssen zuerst ihre Werte erhalten. Dies geschieht in der Vektoreneinbettung. Durch
                “Training”, wo einem LLM viele Textdaten gegeben werden, schaut das LLM jedes einzelne Wort in
                seinem Kontext an und generiert dafür einen Vektor der die Bedeutung durch die Abhängigkeit und
                Ähnlichkeit zu anderen Wörtern erlangt.</p>
            </div>

            </div>
        </section>

        <section id="attention">
            <h2 class="gradient-text">Attention</h2>
            <h3 id="attention-detail">Attention</h3>
            <p>Im Einbettungsschritt haben wir den Input ins LLM in verschiedenen Token aufgetrennt und für jeden
                Token ein Vektor erhalten. Dies ist jedoch noch sehr linear, das Wort Flugzeug wird immer als
                gleicher Vektor interpretiert. Die unterschiedlichen Hersteller oder Typen können nicht
                widergespiegelt werden. Damit die Bedeutung des Wortes genauer definiert werden kann, muss der
                Kontext vom Text angeschaut werden. In der Attention können die anderen Tokens, welche auch
                eingebettet wurden, den Vektor für “Flugzeug” beeinflussen. Somit kann Kontext vom Rest des Textes
                zu einem Wort hinzugefügt werden. In diesem Abschnitt werden wir nur die Self-Attention genauer
                anschauen, die Cross-Attention lassen wir aus.</p>

            <h4>Query und Key Vektoren</h4>
            <p>Innerhalb der Attention wird mit jedem Token zwei <a href="/math/index.html#matrix-vektor"
                    class="text-indigo-600 hover:underline">Matrixmultiplikationen</a> durchgeführt. Beide Matrizen
                bestehen komplett aus Modelparameter, welche, während dem Lernen der LLM definiert werden. Als
                Ergebnis der beiden Multiplikationen erhalten wir Query und Key Vektoren, diese befinden sich in
                einer kleineren Dimension im Vergleich zum ursprünglichen Einbettungsraum.</p>
            <p>Die erste Matrixmultiplikation ergibt einen Query-Vektor, welcher eine Frage an die umliegenden
                Tokens widerspiegelt. Ein sehr stark vereinfachtes Beispiel ist, dass die Matrik, welche bei der
                Matrixmultiplikation für den Query-Vektor verwendet wird, bei Nomen den Fragenvektor “Gibt es ein
                Flugzeug Typ vor mir?” zurückgibt.</p>
            <p>Bei der zweiten Matrixmultiplikation erhalten wir einen Key-Vektor. Der Key-Vektor ist die Antwort
                zur Frage des Query-Vektors. Hier würden in unserem Beispiel Tokens, welche einen Flugzeug Typ
                repräsentieren, die Antwort geben “Ich bin ein Flugzeug Typ, an der Position X”.</p>

            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="diagram-column">
                        <!-- Input -->
                        <div class="node rect input" style="border-width: 2px;">Input Token</div>

                        <!-- Split Arrows (Visual only) -->
                        <div style="font-size: 20px; color: #ccc;">↓</div>

                        <div class="diagram-row" style="align-items: flex-start;">
                            <!-- Query Path -->
                            <div class="diagram-column">
                                <div class="node rect" style="border-style: dashed; background: #f9f9f9;">W_Query</div>
                                <div style="font-size: 20px; color: #999;">×</div>
                                <div class="node rect output"
                                    style="background: linear-gradient(135deg, #a777d1 0%, #4d25fc 100%); color: white;">
                                    Query Vector</div>
                                <div class="diagram-label">q = x · W_q</div>
                            </div>

                            <!-- Key Path -->
                            <div class="diagram-column">
                                <div class="node rect" style="border-style: dashed; background: #f9f9f9;">W_Key</div>
                                <div style="font-size: 20px; color: #999;">×</div>
                                <div class="node rect highlight" style="color: #4d25fc;">Key Vector</div>
                                <div class="diagram-label">k = x · W_k</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h4>Skalarprodukt vom Key und Query Vektor</h4>
            <p>Sobald für jeden Token ein Key und Query-Vektor berechnet wurde, wird das <a
                    href="/math/index.html#skalarprodukt" class="text-indigo-600 hover:underline">Skalarprodukt</a> von
                jedem
                Key-Vektor mit jedem Query-Vektor ausgerechnet. Wenn das Skalarprodukt von einem Query-Vektor und
                einem Key-Vektor stark positiv ist, bedeutet dies, dass das Token vom Key-Vektor auf das Token vom
                Query-Vektor Einfluss haben sollte. Bei einem Skalarprodukt nahe bei Null oder im Minus beeinflussen
                sich die Token gegenseitig nicht oder nur sehr schwach. Bei unserem Beispiel würde das Skalarprodukt
                von einem Flugzeug Typ darstellendem Key-Vektor und einem “Flugzeug” Query-Vektor ein sehr hohes
                Skalarprodukt ergeben und somit aufzeigen, dass ein Flugzeug Typ das Wort “Flugzeug” stark
                beeinflusst.</p>

            <h4>Softmax</h4>
            <p>Sämtliche Skalarprodukte von Key-Vektoren, welche nach einem Query-Vektor in der Reihenfolge sind,
                werden auf minus Unendlich gesetzt. Dies bewirkt, dass in folgendem Beispielsatz: “Das A320 Flugzeug
                neben dem PC-21” das Token für “PC-21” das Token für “Flugzeug” nicht beeinflusst.</p>
            <p>Anschliessend wird ein <a href="/math/index.html#softmax"
                    class="text-indigo-600 hover:underline">Softmax</a> von allen Skalaren von einem Query-Vektor und
                allen Key-Vektoren
                genommen. Softmax setzt alle Werte im Minus Bereich auf Null und gewichtet alle positiven Zahlen so,
                dass am Schluss die Summe sämtlicher Zahlen zusammenaddiert Eins ergeben. Das Ergebnis vom Softmax
                ist eine normalisierte Spalte, in welcher abgelesen werden kann welcher Token welchen Token wie fest
                beeinflussen muss.</p>

            <h4>Beeinflussung</h4>
            <p>Nun wissen wir, zum Beispiel, dass “A320” die Einbettung für “Flugzeug” beeinflussen muss. Für die
                eigentlich Beeinflussung muss nun die Einbettung vom "Flugzeug” Token angepasst werden. Hier wird
                eine weitere Matrixmultiplikation gerechnet. Auch bei dieser besteht die Matrix aus Modelparameter,
                die wiederum, während dem Lernen gesetzt werden. Die Einbettung von jedem Token wird mit der Matrik
                multipliziert, draus erhält man einen weiteren Vektor, der aufzeigt, was bei einem anderen Token
                addiert werden muss, um diesen zu beeinflussen. Diesen Vektor nennen wir von nun an
                “Beeinflussungs-Vektor”. Bei unserem Beispiel würde die Multiplikation, der Matrix, mit dem Vektor
                für “A320” ein Beeinflussungs-Vektor ergeben, welcher zu “Flugzeug” addiert werden kann. Das
                Ergebnis daraus wäre “Flugzeug” mit dem Kontext “A320”.</p>

            <p>Da wir mehrere Tokens haben, welche unterschiedlich stark beeinflussen, müssen sämtliche
                Beeinflussungs-Vektoren mit dem Softmax Ergebnis des gleichen Key-Token multipliziert werden. Nun
                können alle Ergebnisse für ein Query-Token zusammenaddiert werden, um einen Vektor zu erhalten,
                welcher die unterschiedlich stark Gewichtete Beeinflussungen beinhaltet. In unserem Beispiel könnte
                also mit einem Vektor zum Token “Flugzeug” den Kontext “weiss, A320, Airbus” dazu addiert werden,
                wobei die einzelnen Kontextwörter unterschiedlich gewichtet wurden.</p>

            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="diagram-row">
                        <!-- Original -->
                        <div class="diagram-column">
                            <div class="node rect" style="border-style: dashed; background: #f0f0f0; color: #666;">
                                Original "Flugzeug"</div>
                        </div>

                        <!-- Plus -->
                        <div class="node circle input"
                            style="width: 40px; height: 40px; background: #4d25fc; color: white; border: none; font-size: 20px;">
                            +</div>

                        <!-- Context -->
                        <div class="diagram-column">
                            <div class="node rect highlight">Kontext "A320"</div>
                        </div>

                        <!-- Arrow -->
                        <div class="connector-arrow" style="border-left-color: #ccc; border-width: 10px;"></div>

                        <!-- Result -->
                        <div class="diagram-column">
                            <div class="node rect output">Angereichertes Token</div>
                            <div class="diagram-label">"Flugzeug" + A320 Info</div>
                        </div>
                    </div>
                </div>
            </div>

            <p>Nun konnten wir erfolgreich Kontext, aufgrund anderen Tokens, zu einem zuvor kontextlosen Token
                hinzufügen.</p>
            </div>

            <div class="tip-box">
                <h3>Multi Headed Attention</h3>
                <p>Sämtliche Schritte, welche nun erklärt wurden, werden in der Fachsprache einen “Head” genannt. In
                    fast allen LLM’s gibt es nicht nur einen “Head” in der Attention, sondern viele aneinander gereihte
                    “Head”. Dies wird dann Multi Headed Attention genannt. Der Hauptgrund, wieso eine Multi Headed
                    Attention durchgeführt wird, ist, dass bei jedem einzelnen “Head” neue Matrizen verwendet werden,
                    welche aus unterschiedlichen Modelparamter bestehen. Somit kann Kontext noch schneller und genauer
                    bei Tokens hinzugefügt werden.</p>
            </div>

            </div>
        </section>

        <section id="mlp">
            <h2 class="gradient-text">Multilayer Perzeptron (MLP)</h2>
            <h3 id="mlp-detail">Multilayer Perzeptron (MLP)</h3>
            <p>Das MLP ist ein Neurales Netzwerk, welches dem LLM ermöglicht Informationen zu speichern. Ein
                Anschauliches Beispiel ist, wenn man ein ChatBot fragt, wie viel Personen in ein Airbus A320 passen.
                Solange das LLM keinen Zugriff aufs Internet hat, muss die Sitzplatzanzahl innerhalb des Models
                gespeichert sein. Genau diesen Schritt übernimmt das MLP. Jeder Vektor, aus der Attention,
                durchläuft die MLP-Schicht parallel und isoliert von sämtlichen anderen Vektoren. Damit wir den
                Aufbau des MLP besser verstehen ignorieren wir die Parallelität und folgen schrittweise einem
                Durchlauf durchs MLP.</p>

            <h4>Input ins MLP (Matrix Multiplikation)</h4>
            <p>Nach der Attention haben wir mehrere Vektoren erhalten, welche von anderen Tokens beeinflusst wurden.
                Da in der MLP-Schicht alle Vektoren parallel und isoliert voneinander verarbeitet werden, werden wir
                zur Vereinfachung nur einen Vektor, welcher den Airbus A320 widerspiegelt, anschauen.</p>
            <p>Als erster Schritt innerhalb des MLPs wird der Vektor aus der Attention mit einer Matrix
                multipliziert. Bei der Matrix handelt es sich um viele Modelparameter, die, während des Lernens der
                LLM, definiert werden. Innerhalb der Matrix betrachten wir nun eine Zeile als einen weiteren Vektor.
                Bei der Matrixmultiplikation erhalten wir somit das Skalarprodukt des Airbus A320 Vektors und einem
                imaginieren Vektor, welcher aus einer Reihe innerhalb der Matrix besteht.</p>

            <div class="nn-architecture-diagram">
                <div class="diagram-container">
                    <!-- Input Vector -->
                    <div class="diagram-column">
                        <span class="diagram-label" style="font-weight:bold;">Input</span>
                        <div class="vector-box">
                            <div class="vector-cell" style="background:#e6e0f8;">x₁</div>
                            <div class="vector-cell">x₂</div>
                            <div class="vector-cell">x₃</div>
                            <div class="vector-cell">x₄</div>
                        </div>
                    </div>

                    <div style="font-size: 24px; color: #999;">×</div>

                    <!-- Matrix -->
                    <div class="diagram-column">
                        <span class="diagram-label" style="font-weight:bold;">Parameter Matrix</span>
                        <div class="matrix-grid">
                            <!-- Row 1 -->
                            <div class="matrix-cell">w1</div>
                            <div class="matrix-cell">w2</div>
                            <div class="matrix-cell">w3</div>
                            <div class="matrix-cell">w4</div>
                            <!-- Row 2 (Highlighted) -->
                            <div class="matrix-cell" style="background:#e6e0f8; color:#4d25fc; font-weight:bold;">w5
                            </div>
                            <div class="matrix-cell" style="background:#e6e0f8; color:#4d25fc; font-weight:bold;">w6
                            </div>
                            <div class="matrix-cell" style="background:#e6e0f8; color:#4d25fc; font-weight:bold;">w7
                            </div>
                            <div class="matrix-cell" style="background:#e6e0f8; color:#4d25fc; font-weight:bold;">w8
                            </div>
                            <!-- Row 3 -->
                            <div class="matrix-cell">w9</div>
                            <div class="matrix-cell">w10</div>
                            <div class="matrix-cell">w11</div>
                            <div class="matrix-cell">w12</div>
                            <!-- Row 4 -->
                            <div class="matrix-cell">w13</div>
                            <div class="matrix-cell">w14</div>
                            <div class="matrix-cell">w15</div>
                            <div class="matrix-cell">w16</div>
                        </div>
                        <span class="diagram-label" style="color:#a777d1;">Projiziert auf Dimensionen</span>
                    </div>

                    <div style="font-size: 24px; color: #999;">=</div>

                    <!-- Result Vector -->
                    <div class="diagram-column">
                        <span class="diagram-label" style="font-weight:bold;">Expansion</span>
                        <div class="vector-box" style="border-color:#a777d1;">
                            <div class="vector-cell">y₁</div>
                            <div class="vector-cell" style="background:#e6e0f8; font-weight:bold;">y₂</div>
                            <div class="vector-cell">y₃</div>
                            <div class="vector-cell">y₄</div>
                        </div>
                    </div>
                </div>
            </div>

            <p>Das Skalarprodukt von zwei Vektoren liefert Informationen darüber, wie Vektoren zueinanderstehen. Ist
                das Ergebnis positiv, bedeutet dies, dass eine Ähnlichkeit zwischen zwei Vektoren besteht. Ein
                Betrag im Minusbereich zeigt auf, dass die Vektoren gegensätzlich sind. Falls das Ergebnis genau
                null beträgt, haben die Vektoren keine Gemeinsamkeiten. Wenn die beiden Vektoren genau
                übereinstimmen, erhält man den Betrag, von einem der beiden Vektoren, im Quadrat. Sind die Vektoren
                genaue Gegenteile erhält man die Quadratzahl im Minus.</p>
            <p>Um unser Beispiel möglichst verständlich zu halten, gehen wir davon aus, dass sowohl der Vektor aus
                der Attention wie auch der Vektor aus der Matrix Einheitsvektoren sind.</p>
            <p>Jeder Vektor aus der Matrix kann als Frage angesehen werden. Diese Matrixmultiplikation ermöglicht
                es, Fragen zu dem Vektor, welchen wir aus der Attention erhalten haben, zu stellen. Anhand des
                Ergebnisses des Skalarproduktes erkennen wir, ob eine Übereinstimmung oder Gegensätzlichkeit
                besteht.</p>

            <h4>ReLU (Aktivierung)</h4>
            <p>Der erste Schritt innerhalb des MLP ergab einen Vektor mit Zahlen. Jede Zahl sagt aus, wie fest eine
                gestellte Frage mit dem ursprünglichen Vektor aus der Attention übereinstimmt. Um zu definieren,
                wann eine Frage zutrifft, und wann diese nicht zutrifft, wird eine nicht lineare Funktion verwendet.
                Diese Funktion ist unterschiedlich je nach LLM. Während unserer Arbeit schauen wir uns die ReLU
                Funktion genauer an.</p>

            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div
                        style="position: relative; width: 300px; height: 200px; border-left: 2px solid #333; border-bottom: 2px solid #333; margin: 20px;">
                        <!-- Grid Background (Optional, simple lines) -->
                        <div
                            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; background-image: linear-gradient(#f0f0f0 1px, transparent 1px), linear-gradient(90deg, #f0f0f0 1px, transparent 1px); background-size: 20px 20px; z-index: 0;">
                        </div>

                        <!-- Labels -->
                        <div style="position: absolute; bottom: -25px; right: 0; font-weight: bold; color: #333;">x
                        </div>
                        <div style="position: absolute; top: -25px; left: -10px; font-weight: bold; color: #333;">y
                        </div>
                        <div
                            style="position: absolute; top: 10%; right: 10%; font-weight: bold; font-family: monospace; color: #4d25fc; background: white; padding: 5px;">
                            f(x) = max(0, x)</div>

                        <!-- Graph Lines -->
                        <!-- x < 0 -->
                        <div
                            style="position: absolute; bottom: 0; left: 0; width: 50%; height: 4px; border-top: 4px dashed #ccc;">
                        </div>

                        <!-- x > 0 -->
                        <div
                            style="position: absolute; bottom: 0; left: 50%; width: 50px; height: 4px; background: #4d25fc; transform-origin: left bottom; transform: rotate(-45deg) scaleX(3); z-index: 1;">
                        </div>

                        <!-- Origin Dot -->
                        <div class="node circle"
                            style="width: 16px; height: 16px; position: absolute; bottom: -8px; left: calc(50% - 8px); border-color: #4d25fc; z-index: 2;">
                        </div>
                        <div style="position: absolute; bottom: -25px; left: 48%; color: #666; font-size: 0.8rem;">0
                        </div>
                    </div>
                </div>
            </div>

            <p>Die ReLU-Funktion ergibt für alle Werte welche kleiner oder gleich null sind null. Bei positiven
                Zahlen gibt die ReLU-Funktion die positive Zahl, ohne Veränderung, wieder zurück. Nach der
                Ausführung der ReLU-Funktion kann abgelesen werden, welche Fragen zutreffen und welche nicht.
                Sämtliche Werte, welche grösser als null sind, treffen zu. Alle, die gleich null sind, treffen nicht
                zu.</p>
            <p>Bei dieser Funktion kommt nun wieder der Bias ins Spiel. Falls der Fragenvektor die Addition der
                beiden Vektoren «Kann fliegen» und «Ist ein Flugzeug» bekommt man bereits einen positiven Wert, wenn
                nur einer der beiden Fragen stimmt. Mit dem Bias gibt es die Möglichkeit, im Nachhinein das
                Skalarprodukt noch anzupassen und zum Beispiel minus 1.9 rechnen. Da ein Skalarprodukt von zwei
                Einheitsvektoren maximal eins ergibt, wenn diese genau übereinstimmen, würde dieser Bias bewirken,
                dass unser Vektor aus der Attention extrem stark, mit den beiden Eigenschaften übereinstimmen muss.
            </p>

            <h4>Zweite Matrix Multiplikation</h4>
            <p>Aus der ReLU-Funktion erhalten wir einen Vektor mit Zahlen von null bis unendlich. Der nächste
                Schritt innerhalb des MLP ist erneut eine Matrixmultiplikation wie im ersten Schritt. Bei dieser
                Multiplikation ist es am besten, wenn man sich die Matrix spaltenweise vorstellt. Jede Spalte hat
                genau die Grösse des ursprünglichen Vektorraums und widerspiegelt darin eine bestimmte Information.
                In unserem Beispiel gäbe es innerhalb der Matrix zum Beispiel eine Spalte, welche die 150-187
                Sitzplätze innerhalb des ursprünglichen Vektorraums widerspiegelt. Falls das Skalarprodukt der
                Frage, ob es sich um einen Airbus A320 handelt, positiv ist, wird die ReLU Funktion den Wert nicht
                anpassen und bei dieser Matrixmultiplikation wird der Vektor der Sitzplätze beim Endergebnis mit
                einbezogen. Solange die ReLU-Funktion für eine Frage Null zurückgibt, wird diese Spalte nicht zum
                Endergebnis dazugerechnet.</p>
            <p>Am Ende dieses Schrittes haben wir wieder einen Vektor, welcher die Grösse des ursprünglichen
                Vektorenraums hat, und sämtliche zusätzlichen Informationen verglichen mit dem Vektor aus der
                Attention enthält.</p>

            <h4>Residual Connection</h4>
            <p>Damit der ursprüngliche Vektor aus der Attention beim Weiteren verarbeiten nicht komplett vergessen
                wird, addiert der letzte Schritt des MLP den Vektor aus der Attention mit dem Vektor, welcher die
                zusätzlichen Fakten enthält.</p>

            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="diagram-row">
                        <!-- Attention Vector -->
                        <div class="diagram-column">
                            <div class="node rect" style="border-style: dashed; background: #f0f0f0; color: #666;">
                                Attention Vektor (Original)</div>
                        </div>

                        <!-- Plus -->
                        <div style="font-size: 30px; color: #333; font-weight: bold;">+</div>

                        <!-- Fact Vector -->
                        <div class="diagram-column">
                            <div class="node rect highlight" style="background: #e6e0f8; color: #4d25fc;">Fakten Vektor
                                (Neu)</div>
                        </div>

                        <!-- Arrow -->
                        <div class="connector-arrow" style="border-left-color: #4d25fc; border-width: 10px;"></div>

                        <!-- Result -->
                        <div class="diagram-column">
                            <div class="node rect output">Ergebnis</div>
                            <div class="diagram-label">Kombiniertes Wissen</div>
                        </div>
                    </div>
                </div>
            </div>

            <p>In unserem Beispiel hätten wir nun einen Vektor, welcher Airbus, A320 und nun auch die 150-187
                Sitzplätze beinhaltet.</p>
        </section>

        <section id="training">
            <h2 class="gradient-text">Training</h2>
            <h3 id="training-detail">Erklärung Training</h3>
            <p>Wenn man bei einem neuronalen Netzwerk «lernt» sagt, meint man damit eigentlich nichts anderes, als
                die Parameter zu ändern, sodass man ein gewünschtes Ergebnis bekommt. Dies ist jedoch nicht trivial,
                denn heutige LLMs haben über eine Billion Parameter. Man kann sich diese wie viele Schalter
                vorstellen, die man beliebig drehen kann, um einen anderen Wert zu erhalten.</p>

            <h4>Backpropagation</h4>
            <p>Backpropagation ist der zentrale Algorithmus, mit dem ein neuronales Netzwerk lernt.</p>
            <p>Vereinfacht funktioniert der Algorithmus so, dass man einen Satz hat, den man kennt.</p>
            <p>Als Beispiel nehmen wir: «Ein Kirschbaum ist pink.» Jetzt füttern wir das Netzwerk mit diesem Satz,
                lassen jedoch das letzte Wort weg. Das heisst wir fragen das Netzwerk, was das nächste Wort im Satz
                «ein Kirschbaum ist» ist.</p>
            <p>Das Netzwerk wird jetzt durch die Schritte gehen, also Embedding, Attention, MLP und so weiter. Am
                Ende wird es eine Vorhersage machen, die könnte wie folgt aussehen:</p>
            <ul>
                <li>45% pink</li>
                <li>20% gross</li>
                <li>20% wunderschön</li>
                <li>15% weiss</li>
            </ul>
            <p>Das Netzwerk hat «pink» mit 45% als wahrscheinlichstes nächstes Wort gewählt – Das ist richtig,
                jedoch mit 45% nicht überzeugend. Der Verlust misst nun, wie weit die Vorhersage von der Wahrheit
                entfernt ist. Mathematisch nutzen wir dafür die Cross-Entropy: Je höher die vorhergesagte
                Wahrscheinlichkeit für das richtige Wort, desto kleiner der Verlust. In unserem Fall ist die
                Wahrscheinlichkeit für «pink» 45%, also ist der Verlust deutlich grösser als null, in unserem
                Beispiel 55%.</p>



            <p>Das Ziel: Diesen Verlust minimieren.</p>

            <h4>Lernen im Detail</h4>
            <p>Doch wie funktioniert das Lernen im Detail? Um das zu verstehen, hilft ein einfacheres Beispiel.
                Stell dir ein winziges neuronales Netzwerk vor mit nur drei Schichten: zehn Input-Neuronen für die
                zehn häufigsten Wörter, vier Neuronen in einer versteckten Schicht und zwei Output-Neuronen für die
                Kategorien «Säugetier» und «Vogel».</p>

            <!-- Visualization: Small Network -->
            <!-- Visualization: Small Network -->
            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="architecture-container"
                        style="max-width: 500px; display: grid; grid-template-columns: repeat(3, 1fr); position: relative;">
                        <!-- Connection Layer (SVG) -->
                        <svg class="connection-layer"
                            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 0; pointer-events: none;"
                            preserveAspectRatio="none">
                            <!-- Inputs: 16.7% at 24.7%, 43.1%, 61.4%, 95.9% -->
                            <!-- Hiddens: 50.0% at 27.8%, 49.5%, 71.2%, 92.9% -->
                            <!-- Outputs: 83.3% at 42.0%, 79.0% -->

                            <!-- Input 1 -> All Hiddens -->
                            <line x1="16.7%" y1="24.7%" x2="50%" y2="27.8%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="24.7%" x2="50%" y2="49.5%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="24.7%" x2="50%" y2="71.2%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="24.7%" x2="50%" y2="92.9%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />

                            <!-- Input 2 -> All Hiddens -->
                            <line x1="16.7%" y1="43.1%" x2="50%" y2="27.8%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="43.1%" x2="50%" y2="49.5%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="43.1%" x2="50%" y2="71.2%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="43.1%" x2="50%" y2="92.9%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />

                            <!-- Input 3 -> All Hiddens -->
                            <line x1="16.7%" y1="61.4%" x2="50%" y2="27.8%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="61.4%" x2="50%" y2="49.5%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="61.4%" x2="50%" y2="71.2%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="61.4%" x2="50%" y2="92.9%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />

                            <!-- Input 4 (bottom) -> All Hiddens -->
                            <line x1="16.7%" y1="95.9%" x2="50%" y2="27.8%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="95.9%" x2="50%" y2="49.5%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="95.9%" x2="50%" y2="71.2%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="16.7%" y1="95.9%" x2="50%" y2="92.9%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />


                            <!-- Hiddens -> Output 1 (42.0%) -->
                            <line x1="50%" y1="27.8%" x2="83.3%" y2="42.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="49.5%" x2="83.3%" y2="42.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="71.2%" x2="83.3%" y2="42.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="92.9%" x2="83.3%" y2="42.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />

                            <!-- Hiddens -> Output 2 (79.0%) -->
                            <line x1="50%" y1="27.8%" x2="83.3%" y2="79.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="49.5%" x2="83.3%" y2="79.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="71.2%" x2="83.3%" y2="79.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                            <line x1="50%" y1="92.9%" x2="83.3%" y2="79.0%" stroke="#ccc" stroke-width="1"
                                opacity="0.3" />
                        </svg>

                        <!-- Input -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <span class="diagram-label" style="margin-bottom: 10px; font-weight: bold;">Input
                                (Wörter)</span>
                            <div class="node circle input" style="width: 20px; height: 20px;"></div>
                            <div class="node circle input" style="width: 20px; height: 20px;"></div>
                            <div class="node circle input" style="width: 20px; height: 20px;"></div>
                            <div style="font-weight: bold; color: #333;">...</div>
                            <div class="node circle input" style="width: 20px; height: 20px;"></div>
                        </div>

                        <!-- Hidden -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <span class="diagram-label" style="margin-bottom: 10px; font-weight: bold;">Hidden
                                Layer</span>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                        </div>

                        <!-- Output -->
                        <div class="layer-group" style="justify-content: center; height: 100%; gap: 40px;">
                            <span class="diagram-label" style="margin-bottom: 10px; font-weight: bold;">Output
                                (Kategorie)</span>
                            <div class="diagram-column" style="gap: 5px;">
                                <div class="node circle output"
                                    style="width: 40px; height: 40px; background: white; border-color: #4d25fc;"></div>
                                <span class="diagram-label" style="font-size: 0.8rem;">Säugetier</span>
                            </div>
                            <div class="diagram-column" style="gap: 5px;">
                                <div class="node circle output"
                                    style="width: 40px; height: 40px; background: white; border-color: #4d25fc;"></div>
                                <span class="diagram-label" style="font-size: 0.8rem;">Vogel</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <p>Wir trainieren es damit, das Wort «Katze» als Säugetier zu klassifizieren. Im Forward-Pass fliesst
                die Aktivierung durch das Netz, am Ende kommt bei «Säugetier» ein Wert von 0,7 heraus – richtig,
                aber unsicher. Der Verlust beträgt -log(0,7) = 0,15.</p>

            <!-- Visualization: Network with values -->
            <!-- Visualization: Network with values -->
            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div class="architecture-container"
                        style="max-width: 500px; display: grid; grid-template-columns: repeat(3, 1fr); position: relative;">
                        <!-- Connection Layer (SVG) -->
                        <svg class="connection-layer"
                            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 0; pointer-events: none;"
                            preserveAspectRatio="none">
                            <!-- Input (16.7%, 66.8%) to Hidden (50% at 14.8%, 50.0%, 85.2%) -->
                            <line x1="16.7%" y1="66.8%" x2="50%" y2="14.8%" stroke="#ccc" stroke-width="1" />
                            <line x1="16.7%" y1="66.8%" x2="50%" y2="50.0%" stroke="#ccc" stroke-width="1" />
                            <line x1="16.7%" y1="66.8%" x2="50%" y2="85.2%" stroke="#ccc" stroke-width="1" />

                            <!-- Hidden to Output (83.3% at 13.2%, 74.2%) -->
                            <!-- From Hidden 1 (14.8%) -->
                            <line x1="50%" y1="14.8%" x2="83.3%" y2="13.2%" stroke="#ccc" stroke-width="1" />
                            <line x1="50%" y1="14.8%" x2="83.3%" y2="74.2%" stroke="#ccc" stroke-width="1" />

                            <!-- From Hidden 2 (50.0%) -->
                            <line x1="50%" y1="50.0%" x2="83.3%" y2="13.2%" stroke="#ccc" stroke-width="1" />
                            <line x1="50%" y1="50.0%" x2="83.3%" y2="74.2%" stroke="#ccc" stroke-width="1" />

                            <!-- From Hidden 3 (85.2%) -->
                            <line x1="50%" y1="85.2%" x2="83.3%" y2="13.2%" stroke="#ccc" stroke-width="1" />
                            <line x1="50%" y1="85.2%" x2="83.3%" y2="74.2%" stroke="#ccc" stroke-width="1" />
                        </svg>

                        <!-- Input -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <span class="diagram-label" style="margin-bottom: 10px; font-weight: bold;">Input
                                "Katze"</span>
                            <div class="node circle input"
                                style="width: 20px; height: 20px; background: #4d25fc; border-color: #4d25fc;"></div>
                        </div>

                        <!-- Hidden -->
                        <div class="layer-group" style="justify-content: center; height: 100%;">
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                        </div>

                        <!-- Output -->
                        <div class="layer-group" style="justify-content: center; gap: 40px; height: 100%;">
                            <!-- Saugetier -->
                            <div class="diagram-column" style="gap: 5px;">
                                <div class="node circle output" style="width: 44px; height: 44px; font-size: 0.8rem;">
                                    0.7</div>
                                <span class="diagram-label" style="font-size: 0.8rem;">Säugetier</span>
                            </div>
                            <!-- Vogel -->
                            <div class="diagram-column" style="gap: 5px;">
                                <div class="node circle output"
                                    style="width: 40px; height: 40px; background: white; border-color: #4d25fc; color: #4d25fc; font-size: 0.8rem;">
                                    0.3</div>
                                <span class="diagram-label" style="font-size: 0.8rem;">Vogel</span>
                            </div>

                            <!-- Loss Box -->
                            <div
                                style="position: absolute; right: -120px; top: 80%; background: rgba(255, 77, 77, 0.1); border: 1px solid #ff4d4d; color: #d00; padding: 5px 10px; border-radius: 15px; font-weight: bold; font-size: 0.8rem;">
                                Loss = 0.15
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <p>Jetzt beginnt der Backward-Pass: Dieser Fehler von 0,15 wandert rückwärts durch jede einzelne
                Verbindung. Für jedes Gewicht berechnen wir, wie stark es zum Fehler beigetragen hat, also wie sehr
                eine Änderung an diesem Gewicht das Endergebnis ändern würde.</p>
            <p>Die Kettenregel ist das mathematische Rückgrat des Backward-Passes. Sie funktioniert wie eine
                Abstiegsreise mit Zwischenstationen: Der Gesamtfehler am Ende (0,15) hängt vom Output-Wert ab, der
                wiederum vom vorletzten Gewicht abhängt, dieses vom davorliegenden Neuron, und so weiter bis zum
                ersten Gewicht am Input. Die Kettenregel sagt nun: Um zu wissen, wie sehr sich der Fehler ändert,
                wenn wir ein einzelnes Gewicht am Anfang verändern, multiplizieren wir die Ableitungen aller
                Zwischenstationen miteinander.</p>
            <p>Konkret im Netzwerk: Der Fehler von 0,15 hängt direkt vom Output-Wert (0,7) ab. Der Output-Wert hängt
                von der Summe der Hidden-Neuronen ab, die wiederum von den Input-Gewichten für «Katze» abhängen. Die
                Kettenregel multipliziert nun für jede Verbindung die lokalen Ableitungen – also: Wie stark ändert
                sich die Aktivierung dieser Schicht, wenn ich das Gewicht davor verändere? Durch das geschickte
                Zwischenspeichern dieser Teilableitungen während des Forward-Passes kann der Algorithmus im
                Backward-Pass in einem einzigen Sweep von hinten nach vorne alle Gradienten berechnen, ohne für
                jedes Gewicht einen separaten Durchlauf zu starten.</p>

            <!-- Visualization: Backpropagation Arrows -->
            <!-- Visualization: Backpropagation Arrows -->
            <div class="nn-architecture-diagram">
                <div class="html-diagram">
                    <div style="position: relative; padding: 20px;">
                        <!-- Faded Network Background -->
                        <div class="architecture-container"
                            style="max-width: 500px; display: grid; grid-template-columns: repeat(3, 1fr);  filter: grayscale(100%); opacity: 0.4;">
                            <!-- Connection Layer (SVG) -->
                            <svg class="connection-layer"
                                style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 0; pointer-events: none;"
                                preserveAspectRatio="none">
                                <!-- Input (16.7%, 50.0%) to Hidden (50% at 17.3%, 82.7%) -->
                                <line x1="16.7%" y1="50.0%" x2="50%" y2="17.3%" stroke="#ccc" stroke-width="1" />
                                <line x1="16.7%" y1="50.0%" x2="50%" y2="82.7%" stroke="#ccc" stroke-width="1" />

                                <!-- Hidden to Output (83.3%, 50.0%) -->
                                <line x1="50%" y1="17.3%" x2="83.3%" y2="50.0%" stroke="#ccc" stroke-width="1" />
                                <line x1="50%" y1="82.7%" x2="83.3%" y2="50.0%" stroke="#ccc" stroke-width="1" />
                            </svg>

                            <div class="layer-group" style="justify-content: center; height: 100%;">
                                <div class="node circle input" style="width: 20px; height: 20px; background: #4d25fc;">
                                </div>
                            </div>
                            <div class="layer-group" style="justify-content: center; height: 100%;">
                                <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                                <div class="node circle hidden" style="width: 30px; height: 30px;"></div>
                            </div>
                            <div class="layer-group" style="justify-content: center; height: 100%;">
                                <div class="node circle output" style="width: 40px; height: 40px;"></div>
                            </div>
                        </div>

                        <!-- Backward Arrow Overlay (Simulated with absolute div) -->
                        <div
                            style="position: absolute; top: 50%; left: 0; width: 100%; text-align: center; pointer-events: none;">
                            <div
                                style="font-size: 40px; color: #ff4d4d; font-weight: bold; text-shadow: 0 0 10px white;">
                                ← ← ←
                            </div>
                        </div>
                    </div>
                    <div style="margin-top: 10px; color: #ff4d4d; font-weight: bold; font-size: 14px;">
                        Fehler (Gradient) wird zurückgerechnet
                    </div>
                </div>
            </div>

            <p>Die Gradienten zeigen die Richtung des steilsten Anstiegs. Wir wollen aber den Fehler minimieren,
                also gehen wir in die entgegengesetzte Richtung. Die Lernrate, typischerweise eine kleine Zahl wie
                0,01, bestimmt die Schrittgrösse:</p>
            <p><code>Neues Gewicht = altes Gewicht - Lernrate * Gradient</code></p>
            <p>Ein Gewicht, das den Fehler vergrössert hat, wird verringert; eines, das ihn verringern würde, wird
                erhöht. Dies passiert für alle Parameter gleichzeitig.</p>


            </div>

            <div id="learning">
                <h3>Wie jedoch lernen LLMs?</h3>
                <p>Dieses Prinzip, so einfach es klingt, ist exakt das, was in LLMs passiert. Statt zehn Gewichten hat
                    GPT-3 175 Milliarden Parameter. Statt drei Schichten hat es 96 Transformer-Blöcke. Statt eines
                    einzelnen «Katze»-Beispiels trainiert es gleichzeitig über Tausende von Sätzen. Der Ablauf bleibt
                    1:1 identisch: Forward-Pass durch Embedding, Attention und MLP, Berechnung des Verlustes mit
                    Cross-Entropy, Backward-Pass durch alle 96 Schichten hinunter zu den Embeddings, und schliesslich
                    das Update aller Parameter.</p>

                <!-- Visualization: Comparison -->
                <div class="nn-architecture-diagram">
                    <div class="html-diagram">
                        <div class="architecture-container"
                            style="justify-content: center; gap: 40px; align-items: flex-end;">

                            <!-- Left: Small Net -->
                            <div class="diagram-column">
                                <div class="diagram-label" style="font-weight: bold; margin-bottom: 20px;">Unser
                                    Beispiel</div>
                                <div style="padding: 20px 10px;">
                                    <!-- Mini Net -->
                                    <div class="layer-group" style="gap: 10px;">
                                        <div class="node circle"
                                            style="width: 15px; height: 15px; border-color: #4d25fc;"></div>
                                        <div class="node circle"
                                            style="width: 15px; height: 15px; background: #e6e0f8; border-color: #4d25fc;">
                                        </div>
                                        <div class="node circle"
                                            style="width: 15px; height: 15px; border-color: #4d25fc;"></div>
                                    </div>
                                </div>
                                <div class="diagram-label">~20 Parameter</div>
                            </div>

                            <!-- Arrow -->
                            <div style="margin-bottom: 50px; text-align: center;">
                                <div style="font-size: 30px; color: #ccc;">↝</div>
                                <div style="font-size: 0.8rem; color: #999;">Prinzip gleich,<br>nur skaliert</div>
                            </div>

                            <!-- Right: GPT-3 -->
                            <div class="diagram-column">
                                <div class="diagram-label"
                                    style="font-weight: bold; color: #4d25fc; margin-bottom: 20px;">GPT-3 (LLM)</div>
                                <div style="display: flex; flex-direction: column; align-items: center; gap: 2px;">
                                    <!-- Stack -->
                                    <div class="node rect"
                                        style="width: 100px; height: 20px; background: linear-gradient(90deg, #a777d1, #4d25fc); border: none;">
                                    </div>
                                    <div class="node rect"
                                        style="width: 100px; height: 20px; background: linear-gradient(90deg, #a777d1, #4d25fc); border: none;">
                                    </div>
                                    <div class="node rect"
                                        style="width: 100px; height: 20px; background: linear-gradient(90deg, #a777d1, #4d25fc); border: none;">
                                    </div>
                                    <div style="font-weight: bold; color: #a777d1;">⋮</div>
                                    <div style="color: #666; font-size: 0.7rem;">96 Layer</div>
                                    <div class="node rect"
                                        style="width: 100px; height: 20px; background: linear-gradient(90deg, #a777d1, #4d25fc); border: none;">
                                    </div>
                                    <div class="node rect"
                                        style="width: 100px; height: 20px; background: linear-gradient(90deg, #a777d1, #4d25fc); border: none;">
                                    </div>

                                    <!-- Base -->
                                    <div
                                        style="width: 140px; height: 8px; background: #333; opacity: 0.1; margin-top: 10px; border-radius: 4px;">
                                    </div>
                                </div>
                                <div class="diagram-label" style="font-weight: bold; color: #4d25fc;">175'000'000'000
                                    Parameter</div>
                            </div>

                        </div>
                    </div>
                </div>

                <p>Was bei LLMs jedoch besonders ist: Die Embedding-Matrix selbst lernt mit. Während des Trainings
                    rücken «Kirschbaum» und «Baum» im Vektorraum näher zusammen, während «pink» eine spezielle Beziehung
                    zu «Kirschbaum» entwickelt. Die Attention-Parameter lernen, welche Wörter im Satz wichtig sind. Und
                    weil die oberen Schichten direkt für die Vorhersage verantwortlich sind, lernen sie schneller als
                    die unteren, die allgemeine Sprachmuster erfassen.</p>
                <p>Dieser Zyklus wird nicht einmal oder hundert Mal durchgeführt, sondern Milliarden Mal. Jede Epoche,
                    also jeder Durchlauf durch das gesamte Training, feilt die Parameter ein wenig mehr. Irgendwann wird
                    aus dem 45%-Vorschlag für «pink» eine 99%-Wahrscheinlichkeit. Das Netzwerk hat gelernt, dass
                    Kirschbäume pink sein können – nicht durch Regeln, sondern durch reine Optimierung von Zahlen.</p>
                <p>Backpropagation ist also kein magischer Lernprozess, sondern ein mathematischer
                    Fehlerkorrekturmechanismus, der durch die Kettenregel der Analysis effizient Millionen von
                    Parametern gleichzeitig steuert. Ob zehn Gewichte oder 100 Milliarden – das Prinzip bleibt: Fehler
                    messen, ableiten, anpassen und wiederholen.</p>
            </div>

            <div id="rlhf">
                <h3>Reinforcement Learning from Human Feedback</h3>
                <p>RLHF ist eine Training Methode, bei welcher Menschen die Antwort zu einem gegebenen Prompt schreiben.
                    Anhand dieser Prompts und Antworten wird dann das LLM trainiert. Damit erhält man ein LLM, welches
                    unterandere auch moralische Werte abbilden kann. So kann das LLM zum Beispiel lernen, dass Gewalt
                    keine Lösung für einen verbalen Konflikt ist, wobei, rein faktisch gesehen, Gewalt eine Lösung ist.
                    Diese Trainings Methode ist zeitintensiv und teuer, da viel manuelle Arbeit ins Trainieren
                    einfliesst.</p>
                <p>Um Kosten und Zeit zu sparen, wird nach dem erfolgreichen ersten Training, wie oben beschrieben, das
                    LLM nach einem anderen Vorgehen weitertrainiert. Anstelle, dass das LLM anhand bereits bestehender
                    Antworten trainiert wird, generiert es nun mehrere Antworte auf einen Prompt selbständig. Menschen
                    bewerten dann die generierten Antworten von der besten zur schlechtesten. Das LLM kann dann anhand
                    der unterschiedlichen Bewertungen seine Modelparameter noch feiner einstellen. Durch dieses Vorgehen
                    konnte schon Zeit gespart werden, weiterhin muss jedoch ein Mensch aktiv sämtliche Antworten
                    bewerten.</p>
            </div>

            <div id="summary">
                <h3>Zusammenfassung</h3>
                <h4>Grundprinzip: Next Word Prediction</h4>
                <p>LLMs fungieren im Kern als komplexe probabilistische Systeme. Ihre primäre Aufgabe ist die Next Word
                    Prediction: Basierend auf einer gegebenen Eingabe (Prompt) berechnet das Modell die bedingte
                    Wahrscheinlichkeit für das nachfolgende Token.</p>
                <p>Um repetitive oder deterministische Ausgaben zu vermeiden und eine natürlichere Sprachgenerierung zu
                    ermöglichen, wählt das Modell nicht zwingend das Token mit der höchsten Wahrscheinlichkeit, sondern
                    nutzt Sampling-Verfahren zur Varianzbildung. Dieser Prozess verläuft in einer Schleife, heisst,
                    jedes generierte Output-Token wird unmittelbar in die Eingabesequenz für die nächste Berechnung
                    integriert.</p>

                <h4>Datenrepräsentationen: Vektorisierung und Embedding</h4>
                <p>Da neuronale Netze mathematische Operationen ausführen, ist eine Transformation sprachlicher Eingaben
                    in numerische Werte erforderlich. Dies geschieht durch sogenannte Embeddings.</p>
                <p>Wörter werden hier in hochdimensionale Vektoren übersetzt und in einem Vektorraum positioniert.
                    Semantisch verwandte oder kontextuell ähnliche Begriffe (z.B. Synonyme) weisen in diesem
                    mathematischen Raum eine geringe Distanz zueinander auf. Diese räumliche Nähe ermöglicht es dem
                    Modell, semantische Beziehungen zu erfassen.</p>

                <h4>Die Transformer-Architektur</h4>
                <p>Moderne LLMs basieren auf der Transformer-Architektur, welche sich durch zwei Hauptmechanismen
                    auszeichnet:</p>
                <ul>
                    <li><strong>Attention:</strong> Dieser Mechanismus ermöglicht dem Modell die Kontextualisierung von
                        Informationen. Über Query- und Key-Vektoren wird mit dem Skalarprodukt die Relevanz zwischen
                        einzelnen Tokens berechnet. Ein hoher Skalar indiziert eine starke semantische Beziehung.</li>
                    <li><strong>MLP:</strong> Nach der Kontextanalyse durch die Attention-Layer werden die Informationen
                        im MLP weiterverarbeitet. Dieser Teil des Netzwerks dient primär als faktischer Wissensspeicher.
                        Durch Matrixmultiplikationen werden kontextuelle Informationen mit gelerntem Weltwissen
                        verknüpft und in den Ergebnisvektor integriert.</li>
                </ul>

                <h4>Training: Optimierung durch Backpropagation</h4>
                <p>Der Lernprozess eines LLMs entspricht der ständigen Optimierung seiner Gewichte. Das Verfahren nennt
                    man hier die Backpropagation:</p>
                <ul>
                    <li><strong>Forward Pass:</strong> Das Modell generiert eine Vorhersage.</li>
                    <li><strong>Loss Calculation:</strong> Das System vergleicht die Vorhersage des Modells mit der
                        tatsächlich korrekten Antwort.</li>
                    <li><strong>Backward Pass:</strong> Der berechnete Fehler wird nun rückwärts durch das Netzwerk
                        geleitet. Dabei analysiert der Algorithmus, welche Gewichte massgeblich zu diesem Fehler
                        beigetragen haben. Die Abweichung zwischen beiden wird als "Loss" berechnet.</li>
                    <li><strong>Parameter Update:</strong> Die Gewichte werden minimal angepasst. Das Ziel ist es, die
                        Einstellungen so zu verändern, dass das Modell bei einer ähnlichen Aufgabe in Zukunft einen
                        geringeren Fehler macht.</li>
                </ul>
            </div>
        </section>
    </main>

    <div id="footer-placeholder"></div>

    <script src="/js/main.js"></script>
    <script src="/js/footer.js"></script>
    <script src="/js/neural_network.js"></script>
</body>

</html>